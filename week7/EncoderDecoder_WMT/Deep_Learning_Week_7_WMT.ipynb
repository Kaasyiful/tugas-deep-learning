{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIDeUs_Qv5Wh"
      },
      "source": [
        "# Pengerjaan Tugas Deep Learning menggunakan PyTorch and TensorFlow (Week 7, WMT Dataset Menggunakan Encoder-to-Decoder LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FekvojU1P2g4"
      },
      "source": [
        "## Persiapan: Instalasi library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Memastikan Instalasi library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-tuner in /home/husain/dlvenv/lib/python3.12/site-packages (1.4.7)\n",
            "Requirement already satisfied: imbalanced-learn in /home/husain/dlvenv/lib/python3.12/site-packages (0.13.0)\n",
            "Requirement already satisfied: datasets in /home/husain/dlvenv/lib/python3.12/site-packages (3.5.0)\n",
            "Requirement already satisfied: optuna in /home/husain/dlvenv/lib/python3.12/site-packages (4.3.0)\n",
            "Requirement already satisfied: transformers in /home/husain/dlvenv/lib/python3.12/site-packages (4.51.3)\n",
            "Requirement already satisfied: keras in /home/husain/dlvenv/lib/python3.12/site-packages (from keras-tuner) (3.9.2)\n",
            "Requirement already satisfied: packaging in /home/husain/dlvenv/lib/python3.12/site-packages (from keras-tuner) (25.0)\n",
            "Requirement already satisfied: requests in /home/husain/dlvenv/lib/python3.12/site-packages (from keras-tuner) (2.32.3)\n",
            "Requirement already satisfied: kt-legacy in /home/husain/dlvenv/lib/python3.12/site-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /home/husain/dlvenv/lib/python3.12/site-packages (from imbalanced-learn) (2.1.2)\n",
            "Requirement already satisfied: scipy<2,>=1.10.1 in /home/husain/dlvenv/lib/python3.12/site-packages (from imbalanced-learn) (1.15.2)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /home/husain/dlvenv/lib/python3.12/site-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /home/husain/dlvenv/lib/python3.12/site-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /home/husain/dlvenv/lib/python3.12/site-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /home/husain/dlvenv/lib/python3.12/site-packages (from imbalanced-learn) (3.6.0)\n",
            "Requirement already satisfied: filelock in /home/husain/dlvenv/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /home/husain/dlvenv/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/husain/dlvenv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /home/husain/dlvenv/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /home/husain/dlvenv/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /home/husain/dlvenv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /home/husain/dlvenv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /home/husain/dlvenv/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /home/husain/dlvenv/lib/python3.12/site-packages (from datasets) (3.11.18)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/husain/dlvenv/lib/python3.12/site-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/husain/dlvenv/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /home/husain/dlvenv/lib/python3.12/site-packages (from optuna) (1.15.2)\n",
            "Requirement already satisfied: colorlog in /home/husain/dlvenv/lib/python3.12/site-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /home/husain/dlvenv/lib/python3.12/site-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/husain/dlvenv/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/husain/dlvenv/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /home/husain/dlvenv/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: Mako in /home/husain/dlvenv/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /home/husain/dlvenv/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/husain/dlvenv/lib/python3.12/site-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/husain/dlvenv/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/husain/dlvenv/lib/python3.12/site-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/husain/dlvenv/lib/python3.12/site-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/husain/dlvenv/lib/python3.12/site-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/husain/dlvenv/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/husain/dlvenv/lib/python3.12/site-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/husain/dlvenv/lib/python3.12/site-packages (from requests->keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/husain/dlvenv/lib/python3.12/site-packages (from requests->keras-tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/husain/dlvenv/lib/python3.12/site-packages (from requests->keras-tuner) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/husain/dlvenv/lib/python3.12/site-packages (from requests->keras-tuner) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /home/husain/dlvenv/lib/python3.12/site-packages (from sqlalchemy>=1.4.2->optuna) (3.2.1)\n",
            "Requirement already satisfied: absl-py in /home/husain/dlvenv/lib/python3.12/site-packages (from keras->keras-tuner) (2.2.2)\n",
            "Requirement already satisfied: rich in /home/husain/dlvenv/lib/python3.12/site-packages (from keras->keras-tuner) (14.0.0)\n",
            "Requirement already satisfied: namex in /home/husain/dlvenv/lib/python3.12/site-packages (from keras->keras-tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /home/husain/dlvenv/lib/python3.12/site-packages (from keras->keras-tuner) (3.13.0)\n",
            "Requirement already satisfied: optree in /home/husain/dlvenv/lib/python3.12/site-packages (from keras->keras-tuner) (0.15.0)\n",
            "Requirement already satisfied: ml-dtypes in /home/husain/dlvenv/lib/python3.12/site-packages (from keras->keras-tuner) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/husain/dlvenv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/husain/dlvenv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/husain/dlvenv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /home/husain/dlvenv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/husain/dlvenv/lib/python3.12/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/husain/dlvenv/lib/python3.12/site-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/husain/dlvenv/lib/python3.12/site-packages (from rich->keras->keras-tuner) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/husain/dlvenv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# !pip install numpy matplotlib scikit-learn torch tensorflow\n",
        "# %pip install numpy matplotlib scikit-learn torch tensorflow[and-cuda] keras-tuner nltk imbalanced-learn\n",
        "\n",
        "%pip install keras-tuner imbalanced-learn datasets optuna transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Mengimpor Library yang Dibutuhkan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/husain/dlvenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU available: True\n",
            "WARNING:tensorflow:From /tmp/ipykernel_505995/447492026.py:34: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n",
            "GPU available: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1745685251.747179  505995 gpu_device.cc:2019] Created device /device:GPU:0 with 2248 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
          ]
        }
      ],
      "source": [
        "# Import library yang diperlukan\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, Dropout, Input, BatchNormalization\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional, Concatenate\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from datasets import load_dataset\n",
        "import keras_tuner as kt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import optuna\n",
        "from transformers import AutoTokenizer\n",
        "import time\n",
        "\n",
        "# Memeriksa apakah GPU tersedia dan dapat digunakan oleh PyTorch\n",
        "gpu_available = torch.cuda.is_available()\n",
        "print(f\"GPU available: {gpu_available}\")\n",
        "\n",
        "# Memeriksa apakah GPU tersedia dan dapat digunakan oleh TensorFlow\n",
        "gpu_available = tf.test.is_gpu_available()\n",
        "print(f\"GPU available: {gpu_available}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Mendefinisikan Parameter dan Pre-processing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Memuat dataset\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading dataset...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m dataset = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwmt14\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mde-en\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset loaded with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m training examples.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Take a subset for faster training\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dlvenv/lib/python3.12/site-packages/datasets/load.py:2084\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[39m\n\u001b[32m   2081\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance.as_streaming_dataset(split=split)\n\u001b[32m   2083\u001b[39m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2084\u001b[39m \u001b[43mbuilder_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2085\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2086\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2087\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2088\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2089\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2090\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2092\u001b[39m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[32m   2093\u001b[39m keep_in_memory = (\n\u001b[32m   2094\u001b[39m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance.info.dataset_size)\n\u001b[32m   2095\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dlvenv/lib/python3.12/site-packages/datasets/builder.py:925\u001b[39m, in \u001b[36mDatasetBuilder.download_and_prepare\u001b[39m\u001b[34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[39m\n\u001b[32m    923\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    924\u001b[39m     prepare_split_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_proc\u001b[39m\u001b[33m\"\u001b[39m] = num_proc\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[32m    932\u001b[39m \u001b[38;5;28mself\u001b[39m.info.dataset_size = \u001b[38;5;28msum\u001b[39m(split.num_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.info.splits.values())\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dlvenv/lib/python3.12/site-packages/datasets/builder.py:979\u001b[39m, in \u001b[36mDatasetBuilder._download_and_prepare\u001b[39m\u001b[34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[39m\n\u001b[32m    977\u001b[39m split_dict = SplitDict(dataset_name=\u001b[38;5;28mself\u001b[39m.dataset_name)\n\u001b[32m    978\u001b[39m split_generators_kwargs = \u001b[38;5;28mself\u001b[39m._make_split_generators_kwargs(prepare_split_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m split_generators = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_split_generators\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msplit_generators_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[38;5;66;03m# Checksums verification\u001b[39;00m\n\u001b[32m    982\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verification_mode == VerificationMode.ALL_CHECKS \u001b[38;5;129;01mand\u001b[39;00m dl_manager.record_checksums:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dlvenv/lib/python3.12/site-packages/datasets/packaged_modules/parquet/parquet.py:49\u001b[39m, in \u001b[36mParquet._split_generators\u001b[39m\u001b[34m(self, dl_manager)\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAt least one data file must be specified, but got data_files=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.config.data_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     48\u001b[39m dl_manager.download_config.extract_on_the_fly = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m data_files = \u001b[43mdl_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_and_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m splits = []\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m split_name, files \u001b[38;5;129;01min\u001b[39;00m data_files.items():\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dlvenv/lib/python3.12/site-packages/datasets/download/download_manager.py:326\u001b[39m, in \u001b[36mDownloadManager.download_and_extract\u001b[39m\u001b[34m(self, url_or_urls)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdownload_and_extract\u001b[39m(\u001b[38;5;28mself\u001b[39m, url_or_urls):\n\u001b[32m    311\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Download and extract given `url_or_urls`.\u001b[39;00m\n\u001b[32m    312\u001b[39m \n\u001b[32m    313\u001b[39m \u001b[33;03m    Is roughly equivalent to:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    324\u001b[39m \u001b[33;03m        extracted_path(s): `str`, extracted paths of given URL(s).\u001b[39;00m\n\u001b[32m    325\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.extract(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_or_urls\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dlvenv/lib/python3.12/site-packages/datasets/download/download_manager.py:159\u001b[39m, in \u001b[36mDownloadManager.download\u001b[39m\u001b[34m(self, url_or_urls)\u001b[39m\n\u001b[32m    157\u001b[39m start_time = datetime.now()\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m stack_multiprocessing_download_progress_bars():\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     downloaded_path_or_paths = \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdownload_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl_or_urls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmap_tuple\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDownloading data files\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m duration = datetime.now() - start_time\n\u001b[32m    169\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownloading took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduration.total_seconds()\u001b[38;5;250m \u001b[39m//\u001b[38;5;250m \u001b[39m\u001b[32m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m min\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dlvenv/lib/python3.12/site-packages/datasets/utils/py_utils.py:514\u001b[39m, in \u001b[36mmap_nested\u001b[39m\u001b[34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[39m\n\u001b[32m    511\u001b[39m         batch_size = \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) // num_proc + \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) % num_proc > \u001b[32m0\u001b[39m), \u001b[32m1\u001b[39m)\n\u001b[32m    512\u001b[39m     iterable = \u001b[38;5;28mlist\u001b[39m(iter_batched(iterable, batch_size))\n\u001b[32m    513\u001b[39m mapped = [\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m     \u001b[43m_single_map_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    515\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m hf_tqdm(iterable, disable=disable_tqdm, desc=desc)\n\u001b[32m    516\u001b[39m ]\n\u001b[32m    517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[32m    518\u001b[39m     mapped = [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m mapped_batch \u001b[38;5;129;01min\u001b[39;00m mapped \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m mapped_batch]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dlvenv/lib/python3.12/site-packages/datasets/utils/py_utils.py:401\u001b[39m, in \u001b[36m_single_map_nested\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    398\u001b[39m         k: _single_map_nested((function, v, batched, batch_size, types, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m pbar\n\u001b[32m    399\u001b[39m     }\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     mapped = [\u001b[43m_single_map_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m pbar]\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m    403\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m mapped\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dlvenv/lib/python3.12/site-packages/datasets/utils/py_utils.py:382\u001b[39m, in \u001b[36m_single_map_nested\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    375\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m function(data_struct)\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    377\u001b[39m     batched\n\u001b[32m    378\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m    379\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, types)\n\u001b[32m    380\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mdict\u001b[39m, types)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data_struct)\n\u001b[32m    381\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iter_batched(data_struct, batch_size) \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[32m    384\u001b[39m \u001b[38;5;66;03m# Reduce logging to keep things readable in multiprocessing with tqdm\u001b[39;00m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m logging.get_verbosity() < logging.WARNING:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dlvenv/lib/python3.12/site-packages/datasets/download/download_manager.py:220\u001b[39m, in \u001b[36mDownloadManager._download_batched\u001b[39m\u001b[34m(self, url_or_filenames, download_config)\u001b[39m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m thread_map(\n\u001b[32m    207\u001b[39m         download_func,\n\u001b[32m    208\u001b[39m         url_or_filenames,\n\u001b[32m   (...)\u001b[39m\u001b[32m    216\u001b[39m         tqdm_class=tqdm,\n\u001b[32m    217\u001b[39m     )\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    219\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m url_or_filename \u001b[38;5;129;01min\u001b[39;00m url_or_filenames\n\u001b[32m    222\u001b[39m     ]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dlvenv/lib/python3.12/site-packages/datasets/download/download_manager.py:229\u001b[39m, in \u001b[36mDownloadManager._download_single\u001b[39m\u001b[34m(self, url_or_filename, download_config)\u001b[39m\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_relative_path(url_or_filename):\n\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# append the relative path to the base_path\u001b[39;00m\n\u001b[32m    228\u001b[39m     url_or_filename = url_or_path_join(\u001b[38;5;28mself\u001b[39m._base_path, url_or_filename)\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m out = \u001b[43mcached_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    230\u001b[39m out = tracked_str(out)\n\u001b[32m    231\u001b[39m out.set_origin(url_or_filename)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dlvenv/lib/python3.12/site-packages/datasets/utils/file_utils.py:189\u001b[39m, in \u001b[36mcached_path\u001b[39m\u001b[34m(url_or_filename, download_config, **download_kwargs)\u001b[39m\n\u001b[32m    179\u001b[39m resolved_path = huggingface_hub.HfFileSystem(\n\u001b[32m    180\u001b[39m     endpoint=config.HF_ENDPOINT, token=download_config.token\n\u001b[32m    181\u001b[39m ).resolve_path(url_or_filename)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    183\u001b[39m     output_path = \u001b[43mhuggingface_hub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHfApi\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHF_ENDPOINT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdatasets\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43m__version__\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_datasets_user_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresolved_path\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresolved_path\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresolved_path\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresolved_path\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath_in_repo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[32m    198\u001b[39m     huggingface_hub.utils.RepositoryNotFoundError,\n\u001b[32m    199\u001b[39m     huggingface_hub.utils.EntryNotFoundError,\n\u001b[32m    200\u001b[39m     huggingface_hub.utils.RevisionNotFoundError,\n\u001b[32m    201\u001b[39m     huggingface_hub.utils.GatedRepoError,\n\u001b[32m    202\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dlvenv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dlvenv/lib/python3.12/site-packages/huggingface_hub/hf_api.py:5475\u001b[39m, in \u001b[36mHfApi.hf_hub_download\u001b[39m\u001b[34m(self, repo_id, filename, subfolder, repo_type, revision, cache_dir, local_dir, force_download, proxies, etag_timeout, token, local_files_only, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m   5471\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5472\u001b[39m     \u001b[38;5;66;03m# Cannot do `token = token or self.token` as token can be `False`.\u001b[39;00m\n\u001b[32m   5473\u001b[39m     token = \u001b[38;5;28mself\u001b[39m.token\n\u001b[32m-> \u001b[39m\u001b[32m5475\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5476\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5477\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5478\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5479\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5480\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5481\u001b[39m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5482\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5483\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5484\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5485\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5486\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_dir_use_symlinks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_dir_use_symlinks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5487\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5491\u001b[39m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5493\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5495\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5496\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dlvenv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dlvenv/lib/python3.12/site-packages/huggingface_hub/file_download.py:961\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    941\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[32m    942\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m    943\u001b[39m         local_dir=local_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m    958\u001b[39m         local_files_only=local_files_only,\n\u001b[32m    959\u001b[39m     )\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m961\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m    963\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dlvenv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1112\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1110\u001b[39m Path(lock_path).parent.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1111\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[32m-> \u001b[39m\u001b[32m1112\u001b[39m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1113\u001b[39m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.incomplete\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1115\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1116\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1117\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1118\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1119\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1120\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1121\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1122\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1123\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1124\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(pointer_path):\n\u001b[32m   1125\u001b[39m         _create_symlink(blob_path, pointer_path, new_blob=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dlvenv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1675\u001b[39m, in \u001b[36m_download_to_tmp_and_move\u001b[39m\u001b[34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[39m\n\u001b[32m   1669\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1670\u001b[39m             logger.warning(\n\u001b[32m   1671\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mXet Storage is enabled for this repo, but the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhf_xet\u001b[39m\u001b[33m'\u001b[39m\u001b[33m package is not installed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1672\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mFalling back to regular HTTP download. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1673\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mFor better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1674\u001b[39m             )\n\u001b[32m-> \u001b[39m\u001b[32m1675\u001b[39m         \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1676\u001b[39m \u001b[43m            \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1677\u001b[39m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1678\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1679\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1680\u001b[39m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1681\u001b[39m \u001b[43m            \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1682\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1684\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1685\u001b[39m _chmod_and_move(incomplete_path, destination_path)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dlvenv/lib/python3.12/site-packages/huggingface_hub/file_download.py:449\u001b[39m, in \u001b[36mhttp_get\u001b[39m\u001b[34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[39m\n\u001b[32m    447\u001b[39m new_resume_size = resume_size\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDOWNLOAD_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# filter out keep-alive new chunks\u001b[39;49;00m\n\u001b[32m    451\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dlvenv/lib/python3.12/site-packages/requests/models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dlvenv/lib/python3.12/site-packages/urllib3/response.py:1066\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1064\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1065\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1066\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1068\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[32m   1069\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dlvenv/lib/python3.12/site-packages/urllib3/response.py:955\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt, decode_content, cache_content)\u001b[39m\n\u001b[32m    952\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) >= amt:\n\u001b[32m    953\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decoded_buffer.get(amt)\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    957\u001b[39m flush_decoder = amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[32m    959\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) == \u001b[32m0\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dlvenv/lib/python3.12/site-packages/urllib3/response.py:879\u001b[39m, in \u001b[36mHTTPResponse._raw_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    876\u001b[39m fp_closed = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._fp, \u001b[33m\"\u001b[39m\u001b[33mclosed\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    878\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._error_catcher():\n\u001b[32m--> \u001b[39m\u001b[32m879\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[32m    882\u001b[39m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    887\u001b[39m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[32m    888\u001b[39m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[32m    889\u001b[39m         \u001b[38;5;28mself\u001b[39m._fp.close()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dlvenv/lib/python3.12/site-packages/urllib3/response.py:862\u001b[39m, in \u001b[36mHTTPResponse._fp_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    859\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1()\n\u001b[32m    860\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    861\u001b[39m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m862\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:479\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt > \u001b[38;5;28mself\u001b[39m.length:\n\u001b[32m    477\u001b[39m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[32m    478\u001b[39m     amt = \u001b[38;5;28mself\u001b[39m.length\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[32m    481\u001b[39m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_conn()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/socket.py:707\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    709\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/ssl.py:1252\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1249\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1250\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1251\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1254\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/ssl.py:1104\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1104\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1105\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1106\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Konstanta dan parameter\n",
        "NUM_WORDS = 30000\n",
        "MAXLEN = 1000\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 25\n",
        "\n",
        "# Set seed untuk reproduktivitas\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Memuat dataset\n",
        "print(\"Loading dataset...\")\n",
        "dataset = load_dataset(\"wmt14\", \"de-en\")\n",
        "print(f\"Dataset loaded with {len(dataset['train'])} training examples.\")\n",
        "\n",
        "# Take a subset for faster training\n",
        "data = dataset['train'].shuffle(seed=42).select(range(NUM_WORDS))  # Use a smaller subset for faster training\n",
        "\n",
        "german_texts = [item['translation']['de'] for item in data]\n",
        "english_texts = [item['translation']['en'] for item in data]\n",
        "\n",
        "# Tokenisasi teks\n",
        "german_tokenizer = Tokenizer(num_words=NUM_WORDS, oov_token=\"<unk>\")\n",
        "german_tokenizer.fit_on_texts(german_texts)\n",
        "german_sequences = german_tokenizer.texts_to_sequences(german_texts)\n",
        "english_tokenizer = Tokenizer(num_words=NUM_WORDS, oov_token=\"<unk>\")\n",
        "english_tokenizer.fit_on_texts(english_texts)\n",
        "english_sequences = english_tokenizer.texts_to_sequences(english_texts)\n",
        "\n",
        "# Build vocabularies\n",
        "german_vocab = set(german_tokenizer.word_index.keys())\n",
        "english_vocab = set(english_tokenizer.word_index.keys())\n",
        "german_vocab_size = len(german_vocab)\n",
        "english_vocab_size = len(english_vocab)\n",
        "\n",
        "# Create word-to-index mappings\n",
        "german_word2idx = {word: idx for idx, word in enumerate(sorted(german_vocab))}\n",
        "english_word2idx = {word: idx for idx, word in enumerate(sorted(english_vocab))}\n",
        "\n",
        "# Create index-to-word mappings\n",
        "german_idx2word = {idx: word for word, idx in german_word2idx.items()}\n",
        "english_idx2word = {idx: word for word, idx in english_word2idx.items()}\n",
        "\n",
        "# Convert tokens to indices\n",
        "german_indices = [[german_word2idx[word] for word in seq] for seq in german_sequences]\n",
        "english_indices = [[english_word2idx[word] for word in seq] for seq in english_sequences]\n",
        "\n",
        "# Pad sequences to the same length\n",
        "german_padded = [seq + [german_word2idx['<end>']] * (MAXLEN - len(seq)) for seq in german_indices]\n",
        "english_padded = [seq + [english_word2idx['<end>']] * (MAXLEN - len(seq)) for seq in english_indices]\n",
        "# Convert to numpy arrays\n",
        "german_data = np.array(german_padded)\n",
        "english_data = np.array(english_padded)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Menyiapkan Fungsi Pelatihan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model_pytorch(dataset, model, german_data, english_data, german_vocab_size, english_vocab_size, params):\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        german_data, english_data, test_size=0.2, random_state=42\n",
        "    )\n",
        "    \n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = dataset(X_train, y_train)\n",
        "    test_dataset = dataset(X_test, y_test)\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=params['batch_size'])\n",
        "    \n",
        "    # Initialize model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    \n",
        "    # Loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\n",
        "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
        "    \n",
        "    # Training loop\n",
        "    best_accuracy = 0\n",
        "    history = {'accuracy': [], 'loss': [], 'val_accuracy': [], 'val_loss': []}\n",
        "    for epoch in range(params['epochs']):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        \n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            \n",
        "            # Clear gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(inputs, targets, teacher_forcing_ratio=params['teacher_forcing_ratio'])\n",
        "            \n",
        "            # Reshape outputs and targets for loss calculation\n",
        "            outputs = outputs[:, 1:].reshape(-1, english_vocab_size)\n",
        "            targets = targets[:, 1:].reshape(-1)\n",
        "            \n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, targets)\n",
        "            \n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), params['clip'])\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "            \n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f'Epoch: {epoch+1}/{params[\"epochs\"]}, Batch: {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
        "        \n",
        "        # Evaluation and validation\n",
        "        model.eval()\n",
        "        all_predictions = []\n",
        "        all_targets = []\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in test_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                \n",
        "                outputs = model(inputs, targets, teacher_forcing_ratio=0.0)\n",
        "                \n",
        "                # Get predictions (exclude first token which is <start>)\n",
        "                predictions = outputs[:, 1:].argmax(dim=2)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                # Reshape for evaluation\n",
        "                all_predictions.extend(predictions.cpu().numpy().reshape(-1))\n",
        "                all_targets.extend(targets[:, 1:].cpu().numpy().reshape(-1))\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                predicted = (outputs.data > 0.5).float()\n",
        "                val_total += targets.size(0)\n",
        "                val_correct += (predicted == targets).sum().item()\n",
        "        \n",
        "        # Calculate metrics\n",
        "        # Filter out padding tokens\n",
        "        valid_indices = [i for i, target in enumerate(all_targets) if target != 0]\n",
        "        valid_preds = [all_predictions[i] for i in valid_indices]\n",
        "        valid_targets = [all_targets[i] for i in valid_indices]\n",
        "        \n",
        "        accuracy = accuracy_score(valid_preds, valid_targets)\n",
        "        \n",
        "        print(f'Epoch {epoch+1}/{params[\"epochs\"]} - Validation Accuracy:')\n",
        "        print(f'Accuracy: {accuracy:.4f}')\n",
        "\n",
        "        history['accuracy'].append(accuracy)\n",
        "        history['loss'].append(train_loss / len(train_loader))\n",
        "\n",
        "        val_accuracy = val_correct / val_total\n",
        "        history['val_accuracy'].append(val_accuracy)\n",
        "        history['val_loss'].append(val_loss / len(test_loader))\n",
        "        \n",
        "        # Save the best model\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            torch.save(model.state_dict(), 'best_pytorch_model.pt')\n",
        "            \n",
        "            if best_accuracy >= 0.9:\n",
        "                print(f'Target accuracy of 90% achieved: {best_accuracy:.4f}')\n",
        "    \n",
        "    return model, best_accuracy, valid_preds, valid_targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fungsi untuk melatih model TensorFlow\n",
        "def train_model_tensorflow(model, german_data, english_data, german_vocab_size, english_vocab_size, params):\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        german_data, english_data, test_size=0.2, random_state=42\n",
        "    )\n",
        "    \n",
        "    # Define callbacks\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        'best_tf_model.h5',\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=3,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        [X_train, y_train[:, :-1]],  # Input: encoder_input, decoder_input\n",
        "        y_train[:, 1:],              # Target: decoder_target (shifted by 1)\n",
        "        batch_size=params['batch_size'],\n",
        "        epochs=params['epochs'],\n",
        "        validation_split=0.1,\n",
        "        callbacks=[checkpoint, early_stopping]\n",
        "    )\n",
        "    \n",
        "    # Evaluate the model\n",
        "    # Get predictions on test set\n",
        "    y_pred_prob = model.predict([X_test, y_test[:, :-1]])\n",
        "    y_pred = np.argmax(y_pred_prob, axis=2)\n",
        "    \n",
        "    # Reshape for evaluation\n",
        "    y_pred_flat = y_pred.reshape(-1)\n",
        "    y_test_flat = y_test[:, 1:].reshape(-1)\n",
        "    \n",
        "    # Filter out padding tokens\n",
        "    valid_indices = [i for i, target in enumerate(y_test_flat) if target != 0]\n",
        "    valid_preds = [y_pred_flat[i] for i in valid_indices]\n",
        "    valid_targets = [y_test_flat[i] for i in valid_indices]\n",
        "    \n",
        "    return model, history, valid_preds, valid_targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Menyiapkan Fungsi Evaluasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fungsi untuk evaluasi model\n",
        "def calculate_metrics(y_true, y_pred, y_pred_prob=None):\n",
        "    # Convert one-hot encoded predictions to class labels\n",
        "    if len(y_pred.shape) > 1 and y_pred.shape[1] > 1:\n",
        "        y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "    else:\n",
        "        y_pred_labels = y_pred\n",
        "    \n",
        "    # For multi-class metrics, we'll need to binarize\n",
        "    accuracy = accuracy_score(y_true, y_pred_labels)\n",
        "    \n",
        "    # For binary case or with micro averaging\n",
        "    try:\n",
        "        precision = precision_score(y_true, y_pred_labels, average='micro')\n",
        "        recall = recall_score(y_true, y_pred_labels, average='micro')\n",
        "        f1 = f1_score(y_true, y_pred_labels, average='micro')\n",
        "    except:\n",
        "        precision = 0\n",
        "        recall = 0\n",
        "        f1 = 0\n",
        "    \n",
        "    # ROC AUC only for binary or with OvR approach\n",
        "    auc_score = 0\n",
        "    if y_pred_prob is not None:\n",
        "        try:\n",
        "            # For multiclass, we can use OvR approach\n",
        "            if len(np.unique(y_true)) > 2:\n",
        "                tpr, fpr, _ = roc_curve(y_true, y_pred_prob[:, 1])\n",
        "                auc_score = roc_auc_score(tf.keras.utils.to_categorical(y_true), y_pred_prob, multi_class='ovr')\n",
        "            else:\n",
        "                fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n",
        "                auc_score = roc_auc_score(y_true, y_pred_prob[:, 1] if y_pred_prob.shape[1] > 1 else y_pred_prob)\n",
        "        except:\n",
        "            tpr, fpr, _ = 0\n",
        "            auc_score = 0\n",
        "    \n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'auc': auc_score,\n",
        "        'tpr': tpr,\n",
        "        'fpr': fpr\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Menyiapkan Fungsi Visualisasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fungsi untuk membuat visualisasi hasil\n",
        "# Fungsi untuk membuat visualisasi training history\n",
        "def plot_training_history(history, model_name):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    \n",
        "    best_epoch = history['val_loss'].index(min(history['val_loss']))\n",
        "    best_val_loss = min(history['val_loss'])\n",
        "\n",
        "    # Plot Loss, dengan tanda pada titik terbaik\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history['loss'], label='Train Loss')\n",
        "    plt.plot(history['val_loss'], label='Valid Loss')\n",
        "    plt.axvline(x=best_epoch, color='r', linestyle='--', label='Best Model')\n",
        "    plt.axhline(y=best_val_loss, color='g', linestyle='--', label='Best Val Loss')\n",
        "    plt.title(f'{model_name} - Loss History')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    \n",
        "    # Plot Accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history['val_accuracy'], label='Valid Accuracy')\n",
        "    plt.axvline(x=best_epoch, color='r', linestyle='--', label='Best Model')\n",
        "    plt.axhline(y=history['val_accuracy'][best_epoch], color='g', linestyle='--', label='Val Accuracy at Best Model')\n",
        "    plt.title(f'{model_name} - Accuracy History')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'training_history_tensorflow_{model_name}.png')\n",
        "    plt.show()\n",
        "\n",
        "# Fungsi untuk membuat confusion matrix\n",
        "def plot_confusion_matrix(y_true, y_pred, model_name):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title(f'{model_name} - Confusion Matrix')\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(2)\n",
        "    plt.xticks(tick_marks, ['Negative', 'Positive'])\n",
        "    plt.yticks(tick_marks, ['Negative', 'Positive'])\n",
        "    \n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in np.ndindex(cm.shape):\n",
        "        plt.text(j, i, format(cm[i, j], 'd'),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    \n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'confusion_matrix_tensorflow_{model_name}.png')\n",
        "    plt.show()\n",
        "\n",
        "# Fungsi untuk membuat ROC curve\n",
        "def plot_roc_curve(results_dict):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    \n",
        "    for model_name, result in results_dict.items():\n",
        "        plt.plot(result['fpr'], result['tpr'], label=f'{model_name} (AUC = {result[\"auc\"]:.4f})')\n",
        "    \n",
        "    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve Comparison')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig('roc_curve_comparison_tensorflow.png')\n",
        "    plt.show()\n",
        "\n",
        "# Fungsi untuk membuat bar chart perbandingan metrik\n",
        "def plot_metrics_comparison(results_dict):\n",
        "    metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc']\n",
        "    models = list(results_dict.keys())\n",
        "    \n",
        "    values = {metric: [results_dict[model][metric] for model in models] for metric in metrics}\n",
        "    \n",
        "    plt.figure(figsize=(12, 8))\n",
        "    bar_width = 0.15\n",
        "    index = np.arange(len(models))\n",
        "    \n",
        "    for i, metric in enumerate(metrics):\n",
        "        plt.bar(index + i * bar_width, values[metric], bar_width, \n",
        "                label=metric.capitalize())\n",
        "    \n",
        "    plt.xlabel('Models')\n",
        "    plt.ylabel('Scores')\n",
        "    plt.title('Performance Metrics Comparison')\n",
        "    plt.xticks(index + bar_width * 2, models)\n",
        "    plt.legend()\n",
        "    plt.grid(True, axis='y')\n",
        "    plt.savefig('metrics_comparison_tensorflow.png')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjzFqGDjP2g6"
      },
      "source": [
        "## Model Encoder-to-Decoder LSTM Menggunakan PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Penyusunan Dataset dan Loader untuk PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom PyTorch dataset\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, inputs, targets):\n",
        "        self.inputs = torch.tensor(inputs, dtype=torch.long)\n",
        "        self.targets = torch.tensor(targets, dtype=torch.long)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.targets[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Menyusun Model Encoder-Decoder LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Encoder-Decoder PyTorch\n",
        "class EncoderDecoderLSTM(nn.Module):\n",
        "    def __init__(self, input_vocab_size, output_vocab_size, embed_size, hidden_size, num_layers, dropout=0.2):\n",
        "        super(EncoderDecoderLSTM, self).__init__()\n",
        "        \n",
        "        # Encoder\n",
        "        self.encoder_embedding = nn.Embedding(input_vocab_size, embed_size)\n",
        "        self.encoder_lstm = nn.LSTM(embed_size, hidden_size, num_layers, \n",
        "                                   batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
        "        \n",
        "        # Decoder\n",
        "        self.decoder_embedding = nn.Embedding(output_vocab_size, embed_size)\n",
        "        self.decoder_lstm = nn.LSTM(embed_size, hidden_size, num_layers, \n",
        "                                   batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
        "        self.output_layer = nn.Linear(hidden_size, output_vocab_size)\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "    \n",
        "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
        "        batch_size = source.size(0)\n",
        "        target_len = target.size(1)\n",
        "        target_vocab_size = self.output_layer.out_features\n",
        "        \n",
        "        # Tensor to store outputs\n",
        "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(source.device)\n",
        "        \n",
        "        # Encode the source sequence\n",
        "        encoder_embedded = self.encoder_embedding(source)\n",
        "        _, (hidden, cell) = self.encoder_lstm(encoder_embedded)\n",
        "        \n",
        "        # First input to the decoder is the <start> token\n",
        "        decoder_input = target[:, 0].unsqueeze(1)\n",
        "        \n",
        "        for t in range(1, target_len):\n",
        "            # Use previous hidden and cell states\n",
        "            decoder_embedded = self.decoder_embedding(decoder_input)\n",
        "            decoder_output, (hidden, cell) = self.decoder_lstm(decoder_embedded, (hidden, cell))\n",
        "            prediction = self.output_layer(decoder_output)\n",
        "            \n",
        "            # Store prediction\n",
        "            outputs[:, t, :] = prediction.squeeze(1)\n",
        "            \n",
        "            # Teacher forcing\n",
        "            use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
        "            if use_teacher_forcing:\n",
        "                decoder_input = target[:, t].unsqueeze(1)\n",
        "            else:\n",
        "                # Get the highest predicted token\n",
        "                top1 = prediction.argmax(2)\n",
        "                decoder_input = top1\n",
        "                \n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Menyusun Fungsi Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def objective_pytorch(trial, framework, german_data, english_data, german_vocab_size, english_vocab_size):\n",
        "    # Common hyperparameters\n",
        "    params = {\n",
        "        'embed_size': trial.suggest_int('embed_size', 128, 512, step=64),\n",
        "        'hidden_size': trial.suggest_int('hidden_size', 128, 512, step=64),\n",
        "        'num_layers': trial.suggest_int('num_layers', 1, 3),\n",
        "        'batch_size': trial.suggest_int('batch_size', 16, 128, step=16),\n",
        "        'epochs': 10  # Limit epochs for tuning\n",
        "    }\n",
        "    \n",
        "    # PyTorch specific hyperparameters\n",
        "    params.update({\n",
        "        'dropout': trial.suggest_float('dropout', 0.1, 0.5, step=0.1),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),\n",
        "        'teacher_forcing_ratio': trial.suggest_float('teacher_forcing_ratio', 0.5, 1.0, step=0.1),\n",
        "        'clip': trial.suggest_float('clip', 0.1, 5.0, step=0.1)\n",
        "    })\n",
        "    \n",
        "    _, accuracy, _ = train_model_pytorch(\n",
        "        german_data, english_data, german_vocab_size, english_vocab_size, params\n",
        "    )\n",
        "    \n",
        "    return accuracy\n",
        "\n",
        "# Hyperparameter tuning for PyTorch model\n",
        "print(\"\\nPerforming hyperparameter tuning for PyTorch model...\")\n",
        "study_pytorch = optuna.create_study(direction='maximize')\n",
        "study_pytorch.optimize(\n",
        "    lambda trial: objective_pytorch(trial, 'pytorch', german_data, english_data, german_vocab_size, english_vocab_size),\n",
        "    n_trials=10  # Adjust as needed\n",
        ")\n",
        "\n",
        "best_params_pytorch = study_pytorch.best_params\n",
        "print(f\"Best PyTorch parameters: {best_params_pytorch}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Melatih Model dengan Hyperparameter Terbaik"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add necessary parameters that are not part of the tuning\n",
        "best_params_pytorch.update({\n",
        "    'epochs': 20  # Full training with more epochs\n",
        "})\n",
        "\n",
        "# Train the best PyTorch model\n",
        "print(\"\\nTraining the best PyTorch model...\")\n",
        "model_pytorch, pytorch_accuracy, predictions_pytorch, targets_pytorch = train_model_pytorch(\n",
        "    german_data, english_data, german_vocab_size, english_vocab_size, best_params_pytorch\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Mengevaluasi Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model terbaik dan evaluasi\n",
        "model_pytorch.load_state_dict(torch.load('final_model_pytorch.pt'))\n",
        "\n",
        "# Evaluate the model\n",
        "results_pytorch = calculate_metrics(targets_pytorch, predictions_pytorch)\n",
        "# Print results\n",
        "print(\"PyTorch Model Evaluation Results:\")\n",
        "for metric, value in results_pytorch.items():\n",
        "    print(f\"{metric.capitalize()}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Memvisualisasikan Prediksi dari model yang telah dilatih"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "plot_training_history(history_pytorch, 'pytorch')\n",
        "\n",
        "# Plot confusion matrix\n",
        "plot_confusion_matrix(targets_pytorch, predictions_pytorch, 'pytorch')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Menyimpan Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Menyimpan model\n",
        "#torch.save(final_model_pytorch.state_dict(), 'final_model_pytorch.pt')\n",
        "#print(\"Model disimpan sebagai 'final_model_pytorch.pt'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDTeEKQnwoon"
      },
      "source": [
        "## Model Encoder-to-Decoder LSTM Menggunakan TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Konversi Dataset ke format TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fungsi untuk mengkonversi dataset ke format TensorFlow\n",
        "def prepare_tf_dataset(dataset, batch_size=32, buffer_size=10000):\n",
        "    def gen():\n",
        "        for i in range(len(dataset)):\n",
        "            yield {\n",
        "                'encoder_inputs': dataset[i]['translation']['de'],\n",
        "                'decoder_inputs': dataset[i]['translation']['en']\n",
        "            }\n",
        "    \n",
        "    def map_func(item):\n",
        "        return {\n",
        "            'encoder_inputs': item['encoder_inputs'],\n",
        "            'decoder_inputs': item['decoder_inputs'][:-1]  # Input tanpa token akhir\n",
        "        }, item['decoder_inputs'][1:]  # Target dimulai dari token kedua\n",
        "    \n",
        "    ds = tf.data.Dataset.from_generator(\n",
        "        gen,\n",
        "        output_signature={\n",
        "            'encoder_inputs': tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
        "            'decoder_inputs': tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    ds = ds.map(map_func)\n",
        "    ds = ds.shuffle(buffer_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    \n",
        "    return ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Menyusun Model Encode-to-Decoder LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TensorFlow Encoder-Decoder LSTM model\n",
        "def create_tf_model(input_vocab_size, output_vocab_size, embedding_dim, hidden_dim, num_layers):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(None,))\n",
        "    encoder_embedding = Embedding(input_vocab_size, embedding_dim)(encoder_inputs)\n",
        "    \n",
        "    encoder = encoder_embedding\n",
        "    encoder_states = []\n",
        "    \n",
        "    for i in range(num_layers):\n",
        "        encoder_lstm = LSTM(hidden_dim, return_sequences=(i < num_layers-1), return_state=True,\n",
        "                           name=f'encoder_lstm_{i}')\n",
        "        if i == 0:\n",
        "            encoder_outputs, state_h, state_c = encoder_lstm(encoder)\n",
        "        else:\n",
        "            encoder_outputs, state_h, state_c = encoder_lstm(encoder_outputs)\n",
        "        encoder_states.append([state_h, state_c])\n",
        "    \n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "    decoder_embedding = Embedding(output_vocab_size, embedding_dim)(decoder_inputs)\n",
        "    \n",
        "    decoder_lstms = []\n",
        "    decoder_outputs = decoder_embedding\n",
        "    \n",
        "    for i in range(num_layers):\n",
        "        decoder_lstm = LSTM(hidden_dim, return_sequences=True, return_state=True,\n",
        "                           name=f'decoder_lstm_{i}')\n",
        "        decoder_lstms.append(decoder_lstm)\n",
        "    \n",
        "    # Connect all LSTM layers\n",
        "    for i in range(num_layers):\n",
        "        if i == 0:\n",
        "            decoder_outputs, _, _ = decoder_lstms[i](\n",
        "                decoder_outputs, initial_state=encoder_states[i])\n",
        "        else:\n",
        "            decoder_outputs, _, _ = decoder_lstms[i](decoder_outputs)\n",
        "    \n",
        "    decoder_dense = Dense(output_vocab_size, activation='softmax')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    \n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    \n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Menyusun Fungsi Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def objective_tensorflow(trial, framework, german_data, english_data, german_vocab_size, english_vocab_size):\n",
        "    # Common hyperparameters\n",
        "    params = {\n",
        "        'embed_size': trial.suggest_int('embed_size', 128, 512, step=64),\n",
        "        'hidden_size': trial.suggest_int('hidden_size', 128, 512, step=64),\n",
        "        'num_layers': trial.suggest_int('num_layers', 1, 3),\n",
        "        'batch_size': trial.suggest_int('batch_size', 16, 128, step=16),\n",
        "        'epochs': 10  # Limit epochs for tuning\n",
        "    }\n",
        "    \n",
        "    # TensorFlow specific hyperparameters\n",
        "    params.update({\n",
        "        'dropout': trial.suggest_float('dropout', 0.1, 0.5, step=0.1),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
        "    })\n",
        "    \n",
        "    _, metrics, _ = train_model_tensorflow(\n",
        "        german_data, english_data, german_vocab_size, english_vocab_size, params\n",
        "    )\n",
        "    accuracy = metrics['accuracy']\n",
        "    \n",
        "    return accuracy\n",
        "\n",
        "# Train the best PyTorch model\n",
        "print(\"\\nTraining the best PyTorch model...\")\n",
        "best_pytorch_model, pytorch_accuracy, pytorch_metrics = train_model_tensorflow(\n",
        "    german_data, english_data, german_vocab_size, english_vocab_size, best_params_pytorch\n",
        ")\n",
        "\n",
        "# Hyperparameter tuning for TensorFlow model\n",
        "print(\"\\nPerforming hyperparameter tuning for TensorFlow model...\")\n",
        "study_tf = optuna.create_study(direction='maximize')\n",
        "study_tf.optimize(\n",
        "    lambda trial: objective_tensorflow(trial, 'tensorflow', german_data, english_data, german_vocab_size, english_vocab_size),\n",
        "    n_trials=10  # Adjust as needed\n",
        ")\n",
        "\n",
        "best_params_tf = study_tf.best_params\n",
        "print(f\"Best TensorFlow parameters: {best_params_tf}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Melatih Model dengan Hyperparameter Terbaik"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add necessary parameters that are not part of the tuning\n",
        "best_params_tf.update({\n",
        "    'epochs': 20  # Full training with more epochs\n",
        "})\n",
        "\n",
        "# Train the best TensorFlow model\n",
        "print(\"\\nTraining the best TensorFlow model...\")\n",
        "best_tf_model, history_tf, predictions_tf, targets_tf = train_model_tensorflow(\n",
        "    german_data, english_data, german_vocab_size, english_vocab_size, best_params_tf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Mengevaluasi Model dengan Menghitung Akurasi, Presisi, Recall, F1Squared, ROC, dan AUC-ROC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model terbaik dan evaluasi\n",
        "best_tf_model.load_weights('best_tf_model.h5')\n",
        "\n",
        "# Evaluate the model\n",
        "results_tf = calculate_metrics(targets_tf, predictions_tf)\n",
        "# Print results\n",
        "print(\"TensorFlow Model Evaluation Results:\")\n",
        "for metric, value in results_tf.items():\n",
        "    print(f\"{metric.capitalize()}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Memvisualisasikan Prediksi dari model yang telah dilatih"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotting hasil pelatihan\n",
        "plot_training_history(history_tf.history, 'tensorflow')\n",
        "\n",
        "# Plotting confusion matrix\n",
        "plot_confusion_matrix(targets_tf, predictions_tf, 'tensorflow')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Menyimpan Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Menyimpan model\n",
        "best_tf_model.save('model_tf.keras')\n",
        "print(\"Model disimpan sebagai 'model_tf.keras'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perbandingan antara model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Menghitung perbandingan untuk semua model\n",
        "results_dict = {\n",
        "    'PyTorch': results_pytorch,\n",
        "    'TensorFlow': results_tf\n",
        "}\n",
        "\n",
        "# Plotting ROC curve\n",
        "plot_roc_curve(results_dict)\n",
        "# Plotting perbandingan metrik\n",
        "plot_metrics_comparison(results_dict)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "sEcuhK7kxBfZ"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dlvenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
