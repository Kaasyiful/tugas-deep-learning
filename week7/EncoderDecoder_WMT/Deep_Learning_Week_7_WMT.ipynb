{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIDeUs_Qv5Wh"
      },
      "source": [
        "# Pengerjaan Tugas Deep Learning menggunakan PyTorch and TensorFlow (Week 7, WMT Dataset Menggunakan Encoder-to-Decoder LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FekvojU1P2g4"
      },
      "source": [
        "## Persiapan: Instalasi library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Memastikan Instalasi library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install numpy matplotlib scikit-learn torch tensorflow\n",
        "# %pip install numpy matplotlib scikit-learn torch tensorflow[and-cuda] keras-tuner nltk imbalanced-learn\n",
        "\n",
        "%pip install keras-tuner imbalanced-learn datasets optuna transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Mengimpor Library yang Dibutuhkan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import library yang diperlukan\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, Dropout, Input, BatchNormalization\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional, Concatenate\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from datasets import load_dataset\n",
        "import keras_tuner as kt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import optuna\n",
        "from transformers import AutoTokenizer\n",
        "import time\n",
        "\n",
        "# Memeriksa apakah GPU tersedia dan dapat digunakan oleh PyTorch\n",
        "gpu_available = torch.cuda.is_available()\n",
        "print(f\"GPU available: {gpu_available}\")\n",
        "\n",
        "# Memeriksa apakah GPU tersedia dan dapat digunakan oleh TensorFlow\n",
        "gpu_available = tf.test.is_gpu_available()\n",
        "print(f\"GPU available: {gpu_available}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Mendefinisikan Parameter dan Pre-processing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Konstanta dan parameter\n",
        "NUM_WORDS = 40000\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 25\n",
        "\n",
        "# Set seed untuk reproduktivitas\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Memuat dataset\n",
        "print(\"Loading dataset...\")\n",
        "dataset = load_dataset(\"wmt14\", \"de-en\")\n",
        "print(f\"Dataset loaded with {len(dataset['train'])} training examples.\")\n",
        "\n",
        "# Melihat contoh data\n",
        "print(\"Sample data:\")\n",
        "for i in range(3):\n",
        "    print(f\"German: {dataset['train'][i]['translation']['de']}\")\n",
        "    print(f\"English: {dataset['train'][i]['translation']['en']}\")\n",
        "    print()\n",
        "\n",
        "train_dataset = dataset['train'].select(range(min(NUM_WORDS, len(dataset['train']))))\n",
        "val_dataset = dataset['validation'].select(range(min(5000, len(dataset['validation']))))\n",
        "test_dataset = dataset['test'].select(range(min(5000, len(dataset['test']))))\n",
        "\n",
        "print(f\"Using {len(train_dataset)} samples for training.\")\n",
        "print(f\"Using {len(val_dataset)} samples for validation.\")\n",
        "print(f\"Using {len(test_dataset)} samples for testing.\")\n",
        "\n",
        "# Inisialisasi tokenizer\n",
        "tokenizer_de = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n",
        "tokenizer_en = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n",
        "\n",
        "# Fungsi untuk tokenisasi dan padding\n",
        "def tokenize_and_prepare(examples, src_lang='de', tgt_lang='en', max_length=128):\n",
        "    source_texts = [example['translation'][src_lang] for example in examples]\n",
        "    target_texts = [example['translation'][tgt_lang] for example in examples]\n",
        "    \n",
        "    # Tokenisasi\n",
        "    source_encodings = tokenizer_de(source_texts, truncation=True, max_length=max_length, padding='max_length', return_tensors='pt')\n",
        "    target_encodings = tokenizer_en(target_texts, truncation=True, max_length=max_length, padding='max_length', return_tensors='pt')\n",
        "    \n",
        "    return {\n",
        "        'input_ids': source_encodings.input_ids,\n",
        "        'attention_mask': source_encodings.attention_mask,\n",
        "        'labels': target_encodings.input_ids,\n",
        "        'decoder_attention_mask': target_encodings.attention_mask\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Menyiapkan Fungsi Evaluasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fungsi untuk menghitung metrik evaluasi\n",
        "def calculate_metrics(predictions, targets, binary=False):\n",
        "    metrics = {}\n",
        "    \n",
        "    if binary:\n",
        "        # Binary classification metrics\n",
        "        metrics['accuracy'] = accuracy_score(targets, predictions)\n",
        "        metrics['precision'] = precision_score(targets, predictions)\n",
        "        metrics['recall'] = recall_score(targets, predictions)\n",
        "        metrics['f1'] = f1_score(targets, predictions)\n",
        "        \n",
        "        # ROC AUC\n",
        "        try:\n",
        "            metrics['auc'] = roc_auc_score(targets, predictions)\n",
        "            fpr, tpr, _ = roc_curve(targets, predictions)\n",
        "            metrics['roc'] = (fpr, tpr)\n",
        "        except:\n",
        "            metrics['auc'] = 0\n",
        "            metrics['roc'] = ([], [])\n",
        "    else:\n",
        "        # Multi-class metrics (untuk NMT, ini akan menjadi per-token)\n",
        "        # Ubah menjadi binary task dengan membandingkan prediksi dan target\n",
        "        binary_accuracy = (np.array(predictions) == np.array(targets)).astype(int)\n",
        "        metrics['accuracy'] = np.mean(binary_accuracy)\n",
        "        \n",
        "        # Untuk NMT, presisi, recall dan F1 score kurang relevan per token\n",
        "        # tapi bisa dihitung untuk memberikan informasi\n",
        "        # predictions dan targets harus diubah menjadi one-hot encoding untuk metrik ini\n",
        "        # Untuk kesederhanaan, gunakan accuracy sebagai proxy\n",
        "        metrics['precision'] = metrics['accuracy']\n",
        "        metrics['recall'] = metrics['accuracy']\n",
        "        metrics['f1'] = metrics['accuracy']\n",
        "        metrics['auc'] = metrics['accuracy']  # Placeholder\n",
        "        metrics['roc'] = ([], [])  # Placeholder\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "# Fungsi untuk evaluasi model PyTorch\n",
        "def evaluate_model_pytorch(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            src = batch['input_ids'].to(device)\n",
        "            trg = batch['labels'].to(device)\n",
        "            \n",
        "            output = model(src, trg, 0)  # Turn off teacher forcing\n",
        "            \n",
        "            # Abaikan token awal\n",
        "            output = output[:, 1:].reshape(-1, output.shape[-1])\n",
        "            trg = trg[:, 1:].reshape(-1)\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "            \n",
        "            # Kumpulkan prediksi dan target untuk metrik evaluasi\n",
        "            predictions = output.argmax(1).cpu().numpy()\n",
        "            targets = trg.cpu().numpy()\n",
        "            \n",
        "            # Hanya menggunakan token valid (bukan padding)\n",
        "            valid_indices = targets != 0  # Asumsi 0 adalah padding\n",
        "            all_predictions.extend(predictions[valid_indices])\n",
        "            all_targets.extend(targets[valid_indices])\n",
        "    \n",
        "    # Hitung metrik evaluasi\n",
        "    metrics = calculate_metrics(all_predictions, all_targets)\n",
        "    \n",
        "    return epoch_loss / len(dataloader), metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fungsi untuk evaluasi model TensorFlow\n",
        "def evaluate_model_tf(model, test_data, model_name):\n",
        "    x_test, y_test = test_data\n",
        "    \n",
        "    # Prediksi\n",
        "    y_pred_prob = model.predict(x_test)\n",
        "    y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
        "    \n",
        "    # Menghitung metrik evaluasi\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_pred_prob)\n",
        "    \n",
        "    # Menghitung ROC curve\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
        "    \n",
        "    # Print hasil evaluasi\n",
        "    print(f\"\\nEvaluasi Model {model_name}:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"AUC: {auc:.4f}\")\n",
        "    \n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'auc': auc,\n",
        "        'fpr': fpr,\n",
        "        'tpr': tpr,\n",
        "        'y_pred_prob': y_pred_prob,\n",
        "        'y_pred': y_pred,\n",
        "        'y_test': y_test\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Menyiapkan Fungsi Visualisasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fungsi untuk membuat visualisasi hasil\n",
        "# Fungsi untuk membuat visualisasi training history\n",
        "def plot_training_history(history, model_name):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    \n",
        "    best_epoch = history['val_loss'].index(min(history['val_loss']))\n",
        "    best_val_loss = min(history['val_loss'])\n",
        "\n",
        "    # Plot Loss, dengan tanda pada titik terbaik\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history['loss'], label='Train Loss')\n",
        "    plt.plot(history['val_loss'], label='Valid Loss')\n",
        "    plt.axvline(x=best_epoch, color='r', linestyle='--', label='Best Model')\n",
        "    plt.axhline(y=best_val_loss, color='g', linestyle='--', label='Best Val Loss')\n",
        "    plt.title(f'{model_name} - Loss History')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    \n",
        "    # Plot Accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history['val_accuracy'], label='Valid Accuracy')\n",
        "    plt.axvline(x=best_epoch, color='r', linestyle='--', label='Best Model')\n",
        "    plt.axhline(y=history['val_accuracy'][best_epoch], color='g', linestyle='--', label='Val Accuracy at Best Model')\n",
        "    plt.title(f'{model_name} - Accuracy History')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'training_history_tensorflow_{model_name}.png')\n",
        "    plt.show()\n",
        "\n",
        "# Fungsi untuk membuat confusion matrix\n",
        "def plot_confusion_matrix(y_true, y_pred, model_name):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title(f'{model_name} - Confusion Matrix')\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(2)\n",
        "    plt.xticks(tick_marks, ['Negative', 'Positive'])\n",
        "    plt.yticks(tick_marks, ['Negative', 'Positive'])\n",
        "    \n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in np.ndindex(cm.shape):\n",
        "        plt.text(j, i, format(cm[i, j], 'd'),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    \n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'confusion_matrix_tensorflow_{model_name}.png')\n",
        "    plt.show()\n",
        "\n",
        "# Fungsi untuk membuat ROC curve\n",
        "def plot_roc_curve(results_dict):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    \n",
        "    for model_name, result in results_dict.items():\n",
        "        plt.plot(result['fpr'], result['tpr'], label=f'{model_name} (AUC = {result[\"auc\"]:.4f})')\n",
        "    \n",
        "    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve Comparison')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig('roc_curve_comparison_tensorflow.png')\n",
        "    plt.show()\n",
        "\n",
        "# Fungsi untuk membuat bar chart perbandingan metrik\n",
        "def plot_metrics_comparison(results_dict):\n",
        "    metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc']\n",
        "    models = list(results_dict.keys())\n",
        "    \n",
        "    values = {metric: [results_dict[model][metric] for model in models] for metric in metrics}\n",
        "    \n",
        "    plt.figure(figsize=(12, 8))\n",
        "    bar_width = 0.15\n",
        "    index = np.arange(len(models))\n",
        "    \n",
        "    for i, metric in enumerate(metrics):\n",
        "        plt.bar(index + i * bar_width, values[metric], bar_width, \n",
        "                label=metric.capitalize())\n",
        "    \n",
        "    plt.xlabel('Models')\n",
        "    plt.ylabel('Scores')\n",
        "    plt.title('Performance Metrics Comparison')\n",
        "    plt.xticks(index + bar_width * 2, models)\n",
        "    plt.legend()\n",
        "    plt.grid(True, axis='y')\n",
        "    plt.savefig('metrics_comparison_tensorflow.png')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjzFqGDjP2g6"
      },
      "source": [
        "## Model Encoder-to-Decoder LSTM Menggunakan PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Penyusunan Dataset dan Loader untuk PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class untuk dataset PyTorch\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, examples, src_lang='de', tgt_lang='en', max_length=128):\n",
        "        self.examples = examples\n",
        "        self.src_lang = src_lang\n",
        "        self.tgt_lang = tgt_lang\n",
        "        self.max_length = max_length\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        example = self.examples[idx]\n",
        "        source_text = example['translation'][self.src_lang]\n",
        "        target_text = example['translation'][self.tgt_lang]\n",
        "        \n",
        "        # Tokenisasi\n",
        "        source_tokens = tokenizer_de(source_text, truncation=True, max_length=self.max_length, return_tensors='pt')\n",
        "        target_tokens = tokenizer_en(target_text, truncation=True, max_length=self.max_length, return_tensors='pt')\n",
        "        \n",
        "        return {\n",
        "            'input_ids': source_tokens.input_ids.squeeze(),\n",
        "            'attention_mask': source_tokens.attention_mask.squeeze(),\n",
        "            'labels': target_tokens.input_ids.squeeze(),\n",
        "            'decoder_attention_mask': target_tokens.attention_mask.squeeze()\n",
        "        }\n",
        "\n",
        "# Membuat dataset untuk PyTorch\n",
        "train_dataset_torch = TranslationDataset(train_dataset)\n",
        "val_dataset_torch = TranslationDataset(val_dataset)\n",
        "test_dataset_torch = TranslationDataset(test_dataset)\n",
        "\n",
        "# Collate function untuk DataLoader PyTorch\n",
        "def collate_fn(batch):\n",
        "    input_ids = pad_sequence([item['input_ids'] for item in batch], batch_first=True)\n",
        "    attention_mask = pad_sequence([item['attention_mask'] for item in batch], batch_first=True)\n",
        "    labels = pad_sequence([item['labels'] for item in batch], batch_first=True)\n",
        "    decoder_attention_mask = pad_sequence([item['decoder_attention_mask'] for item in batch], batch_first=True)\n",
        "    \n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_mask,\n",
        "        'labels': labels,\n",
        "        'decoder_attention_mask': decoder_attention_mask\n",
        "    }\n",
        "\n",
        "# DataLoader untuk PyTorch\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset_torch, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset_torch, batch_size=batch_size, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset_torch, batch_size=batch_size, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Menyusun Model Encoder-Decoder LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Encoder-Decoder PyTorch\n",
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, enc_emb_dim, dec_emb_dim, enc_hidden_dim, \n",
        "                 dec_hidden_dim, enc_dropout, dec_dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = Encoder(input_dim, enc_emb_dim, enc_hidden_dim, dec_hidden_dim, enc_dropout, device)\n",
        "        self.decoder = Decoder(output_dim, dec_emb_dim, enc_hidden_dim, dec_hidden_dim, dec_dropout, device)\n",
        "        self.device = device\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        # src: [batch_size, src_len]\n",
        "        # trg: [batch_size, trg_len]\n",
        "        \n",
        "        batch_size = src.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        # Tensor untuk menyimpan output\n",
        "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        # Enkode sumber\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "        \n",
        "        # Dekode target\n",
        "        # Gunakan token awal sebagai input awal decoder\n",
        "        input = trg[:, 0]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
        "            outputs[:, t] = output\n",
        "            \n",
        "            # Putuskan apakah menggunakan prediksi atau target sebenarnya sebagai input berikutnya\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[:, t] if teacher_force else top1\n",
        "            \n",
        "        return outputs\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hidden_dim, dec_hidden_dim, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, enc_hidden_dim, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(enc_hidden_dim * 2, dec_hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.device = device\n",
        "        \n",
        "    def forward(self, src):\n",
        "        # src: [batch_size, src_len]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        # embedded: [batch_size, src_len, emb_dim]\n",
        "        \n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "        # outputs: [batch_size, src_len, enc_hidden_dim * 2]\n",
        "        # hidden: [2, batch_size, enc_hidden_dim]\n",
        "        \n",
        "        # Gabungkan hidden state forward dan backward terakhir\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "        # hidden: [batch_size, enc_hidden_dim * 2]\n",
        "        \n",
        "        hidden = torch.tanh(self.fc(hidden))\n",
        "        # hidden: [batch_size, dec_hidden_dim]\n",
        "        \n",
        "        return outputs, hidden.unsqueeze(0)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hidden_dim, dec_hidden_dim, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.output_dim = output_dim\n",
        "        self.attention = Attention(enc_hidden_dim * 2, dec_hidden_dim)\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim + enc_hidden_dim * 2, dec_hidden_dim, batch_first=True)\n",
        "        self.fc_out = nn.Linear(dec_hidden_dim + enc_hidden_dim * 2 + emb_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.device = device\n",
        "        \n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        # input: [batch_size]\n",
        "        # hidden: [1, batch_size, dec_hidden_dim]\n",
        "        # encoder_outputs: [batch_size, src_len, enc_hidden_dim * 2]\n",
        "        \n",
        "        input = input.unsqueeze(1)  # [batch_size, 1]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # embedded: [batch_size, 1, emb_dim]\n",
        "        \n",
        "        # Perhitungan attention\n",
        "        a = self.attention(hidden, encoder_outputs)\n",
        "        # a: [batch_size, 1, src_len]\n",
        "        \n",
        "        # Weighted sum encoder outputs\n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        # weighted: [batch_size, 1, enc_hidden_dim * 2]\n",
        "        \n",
        "        # Konkatenasi embedding dan weighted untuk input RNN\n",
        "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
        "        # rnn_input: [batch_size, 1, emb_dim + enc_hidden_dim * 2]\n",
        "        \n",
        "        output, hidden = self.rnn(rnn_input, hidden)\n",
        "        # output: [batch_size, 1, dec_hidden_dim]\n",
        "        # hidden: [1, batch_size, dec_hidden_dim]\n",
        "        \n",
        "        # Konkatenasi untuk prediksi\n",
        "        output = torch.cat((output.squeeze(1), weighted.squeeze(1), embedded.squeeze(1)), dim=1)\n",
        "        # output: [batch_size, dec_hidden_dim + enc_hidden_dim * 2 + emb_dim]\n",
        "        \n",
        "        prediction = self.fc_out(output)\n",
        "        # prediction: [batch_size, output_dim]\n",
        "        \n",
        "        return prediction, hidden\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hidden_dim, dec_hidden_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.attn = nn.Linear(enc_hidden_dim + dec_hidden_dim, dec_hidden_dim)\n",
        "        self.v = nn.Linear(dec_hidden_dim, 1, bias=False)\n",
        "        \n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # hidden: [1, batch_size, dec_hidden_dim]\n",
        "        # encoder_outputs: [batch_size, src_len, enc_hidden_dim]\n",
        "        \n",
        "        batch_size = encoder_outputs.shape[0]\n",
        "        src_len = encoder_outputs.shape[1]\n",
        "        \n",
        "        # Ulangi hidden state untuk setiap token sumber\n",
        "        hidden = hidden.permute(1, 0, 2)  # [batch_size, 1, dec_hidden_dim]\n",
        "        hidden = hidden.repeat(1, src_len, 1)  # [batch_size, src_len, dec_hidden_dim]\n",
        "        \n",
        "        # Konkatenasi hidden state dengan encoder outputs\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        # energy: [batch_size, src_len, dec_hidden_dim]\n",
        "        \n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        # attention: [batch_size, src_len]\n",
        "        \n",
        "        return torch.softmax(attention, dim=1).unsqueeze(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Menyusun Fungsi Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fungsi untuk hyperparameter tuning dengan Optuna untuk PyTorch\n",
        "def objective_pytorch(trial):\n",
        "    # Hyperparameters untuk tuning\n",
        "    enc_emb_dim = trial.suggest_int('enc_emb_dim', 128, 512)\n",
        "    dec_emb_dim = trial.suggest_int('dec_emb_dim', 128, 512)\n",
        "    enc_hidden_dim = trial.suggest_int('enc_hidden_dim', 128, 512)\n",
        "    dec_hidden_dim = trial.suggest_int('dec_hidden_dim', 128, 512)\n",
        "    enc_dropout = trial.suggest_float('enc_dropout', 0.1, 0.5)\n",
        "    dec_dropout = trial.suggest_float('dec_dropout', 0.1, 0.5)\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
        "    \n",
        "    # Buat model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    input_dim = len(tokenizer_de.get_vocab())\n",
        "    output_dim = len(tokenizer_en.get_vocab())\n",
        "    \n",
        "    model = EncoderDecoder(input_dim, output_dim, enc_emb_dim, dec_emb_dim, \n",
        "                         enc_hidden_dim, dec_hidden_dim, enc_dropout, dec_dropout, device).to(device)\n",
        "    \n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Abaikan padding\n",
        "    \n",
        "    # Training dengan early stopping\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 3\n",
        "    patience_counter = 0\n",
        "    n_epochs = 10\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        \n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            src = batch['input_ids'].to(device)\n",
        "            trg = batch['labels'].to(device)\n",
        "            \n",
        "            output = model(src, trg)\n",
        "            \n",
        "            # Hilangkan token awal pada output dan target\n",
        "            output = output[:, 1:].reshape(-1, output.shape[-1])\n",
        "            trg = trg[:, 1:].reshape(-1)\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "        train_loss = epoch_loss / len(train_loader)\n",
        "        \n",
        "        # Evaluate on validation set\n",
        "        val_loss, val_metrics = evaluate_model_pytorch(model, val_loader, criterion, device)\n",
        "        \n",
        "        print(f'Epoch: {epoch+1}')\n",
        "        print(f'Train Loss: {train_loss:.3f}')\n",
        "        print(f'Val Loss: {val_loss:.3f}')\n",
        "        print(f'Val Accuracy: {val_metrics[\"accuracy\"]:.3f}')\n",
        "        \n",
        "        # Early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), 'best_model_pytorch.pt')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping triggered!\")\n",
        "                break\n",
        "    \n",
        "    # Load best model for final evaluation\n",
        "    model.load_state_dict(torch.load('best_model_pytorch.pt'))\n",
        "    _, test_metrics = evaluate_model_pytorch(model, test_loader, criterion, device)\n",
        "    \n",
        "    return test_metrics['accuracy']\n",
        "\n",
        "print(\"Running PyTorch model hyperparameter tuning...\")\n",
        "study_pytorch = optuna.create_study(direction='maximize')\n",
        "study_pytorch.optimize(objective_pytorch, n_trials=5)  # Adjust n_trials as needed\n",
        "\n",
        "best_params_pytorch = study_pytorch.best_params\n",
        "print(\"\\nBest PyTorch hyperparameters:\")\n",
        "print(best_params_pytorch)\n",
        "print(f\"Best accuracy: {study_pytorch.best_value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Melatih Model dengan Hyperparameter Terbaik"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train final PyTorch model\n",
        "print(\"\\nTraining final PyTorch model with best hyperparameters...\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "input_dim = len(tokenizer_de.get_vocab())\n",
        "output_dim = len(tokenizer_en.get_vocab())\n",
        "\n",
        "model_pytorch = EncoderDecoder(\n",
        "    input_dim, output_dim, \n",
        "    best_params_pytorch['enc_emb_dim'], \n",
        "    best_params_pytorch['dec_emb_dim'],\n",
        "    best_params_pytorch['enc_hidden_dim'], \n",
        "    best_params_pytorch['dec_hidden_dim'],\n",
        "    best_params_pytorch['enc_dropout'], \n",
        "    best_params_pytorch['dec_dropout'], \n",
        "    device\n",
        ").to(device)\n",
        "\n",
        "optimizer = optim.Adam(model_pytorch.parameters(), lr=best_params_pytorch['learning_rate'])\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "history_pytorch = {'accuracy': [], 'loss': [], 'val_accuracy': [], 'val_loss': []}\n",
        "n_epochs = 20\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # Training\n",
        "    model_pytorch.train()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        src = batch['input_ids'].to(device)\n",
        "        trg = batch['labels'].to(device)\n",
        "        \n",
        "        output = model_pytorch(src, trg)\n",
        "        \n",
        "        output = output[:, 1:].reshape(-1, output.shape[-1])\n",
        "        trg = trg[:, 1:].reshape(-1)\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model_pytorch.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "    \n",
        "    train_loss = epoch_loss / len(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    val_loss, val_metrics = evaluate_model_pytorch(model_pytorch, val_loader, criterion, device)\n",
        "\n",
        "    print(f'Epoch: {epoch+1}')\n",
        "    print(f'Train Loss: {train_loss:.3f}')\n",
        "    print(f'Val Loss: {val_loss:.3f}')\n",
        "    print(f'Val Accuracy: {val_metrics[\"accuracy\"]:.3f}')\n",
        "\n",
        "    history_pytorch['accuracy'].append(val_metrics['accuracy'])\n",
        "    history_pytorch['loss'].append(train_loss)\n",
        "    history_pytorch['val_accuracy'].append(val_metrics['accuracy'])\n",
        "    history_pytorch['val_loss'].append(val_loss)\n",
        "    \n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model_pytorch.state_dict(), 'final_model_pytorch.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Mengevaluasi Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model terbaik dan evaluasi\n",
        "model_pytorch.load_state_dict(torch.load('final_model_pytorch.pt'))\n",
        "test_loss, test_metrics_pytorch = evaluate_model_pytorch(pytorch, test_loader, criterion, device)\n",
        "\n",
        "print(\"\\nFinal PyTorch model evaluation:\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_metrics_pytorch['accuracy']:.4f}\")\n",
        "print(f\"Test Precision: {test_metrics_pytorch['precision']:.4f}\")\n",
        "print(f\"Test Recall: {test_metrics_pytorch['recall']:.4f}\")\n",
        "print(f\"Test F1 Score: {test_metrics_pytorch['f1']:.4f}\")\n",
        "print(f\"Test AUC: {test_metrics_pytorch['auc']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Memvisualisasikan Prediksi dari model yang telah dilatih"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotting hasil pelatihan\n",
        "plot_training_history(history_pytorch.history, \"birnn\")\n",
        "\n",
        "# Plotting confusion matrix\n",
        "plot_confusion_matrix(test_metrics_pytorch['y_test'], test_metrics_pytorch['y_pred'], \"pytorch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Menyimpan Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Menyimpan model\n",
        "#torch.save(final_model_pytorch.state_dict(), 'final_model_pytorch.pt')\n",
        "#print(\"Model disimpan sebagai 'final_model_pytorch.pt'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDTeEKQnwoon"
      },
      "source": [
        "## Model Encoder-to-Decoder LSTM Menggunakan TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Konversi Dataset ke format TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fungsi untuk mengkonversi dataset ke format TensorFlow\n",
        "def prepare_tf_dataset(dataset, batch_size=32, buffer_size=10000):\n",
        "    def gen():\n",
        "        for item in dataset:\n",
        "            yield {\n",
        "                'encoder_inputs': tokenizer_de.encode(item['translation']['de'], max_length=128, truncation=True),\n",
        "                'decoder_inputs': tokenizer_en.encode(item['translation']['en'], max_length=128, truncation=True)\n",
        "            }\n",
        "    \n",
        "    def map_func(item):\n",
        "        return {\n",
        "            'encoder_inputs': item['encoder_inputs'],\n",
        "            'decoder_inputs': item['decoder_inputs'][:-1]  # Input tanpa token akhir\n",
        "        }, item['decoder_inputs'][1:]  # Target dimulai dari token kedua\n",
        "    \n",
        "    ds = tf.data.Dataset.from_generator(\n",
        "        gen,\n",
        "        output_signature={\n",
        "            'encoder_inputs': tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
        "            'decoder_inputs': tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    ds = ds.map(map_func)\n",
        "    ds = ds.shuffle(buffer_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    \n",
        "    return ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Menyusun Model Encode-to-Decoder LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model TensorFlow/Keras untuk NMT\n",
        "def create_tensorflow_model(vocab_size_src, vocab_size_tgt, embedding_dim, hidden_units, dropout_rate):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(None,))\n",
        "    encoder_embedding = Embedding(vocab_size_src, embedding_dim)(encoder_inputs)\n",
        "    encoder_dropout = Dropout(dropout_rate)(encoder_embedding)\n",
        "    encoder_lstm = Bidirectional(LSTM(hidden_units, return_sequences=True, return_state=True))\n",
        "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_lstm(encoder_dropout)\n",
        "    \n",
        "    # Mengkombinasikan state dari kedua arah\n",
        "    state_h = Concatenate()([forward_h, backward_h])\n",
        "    state_c = Concatenate()([forward_c, backward_c])\n",
        "    encoder_states = [state_h, state_c]\n",
        "    \n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "    decoder_embedding = Embedding(vocab_size_tgt, embedding_dim)(decoder_inputs)\n",
        "    decoder_dropout = Dropout(dropout_rate)(decoder_embedding)\n",
        "    \n",
        "    # Menggunakan LSTM yang sama dengan dimensi hidden state yang digandakan karena bidirectional encoder\n",
        "    decoder_lstm = LSTM(hidden_units * 2, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_dropout, initial_state=encoder_states)\n",
        "    \n",
        "    # Attention mechanism\n",
        "    attention = Attention()([decoder_outputs, encoder_outputs])\n",
        "    concat = Concatenate()([decoder_outputs, attention])\n",
        "    \n",
        "    # Output layer\n",
        "    decoder_dense = Dense(vocab_size_tgt, activation='softmax')\n",
        "    decoder_outputs = decoder_dense(concat)\n",
        "    \n",
        "    # Define model\n",
        "    model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Menyusun Fungsi Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fungsi untuk hyperparameter tuning dengan Optuna untuk TensorFlow\n",
        "def objective_tensorflow(trial):\n",
        "    # Hyperparameters untuk tuning\n",
        "    embedding_dim = trial.suggest_int('embedding_dim', 128, 512)\n",
        "    hidden_units = trial.suggest_int('hidden_units', 128, 512)\n",
        "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
        "    \n",
        "    # Persiapkan dataset\n",
        "    train_ds_tf = prepare_tf_dataset(train_dataset)\n",
        "    val_ds_tf = prepare_tf_dataset(val_dataset)\n",
        "    \n",
        "    # Buat model\n",
        "    vocab_size_src = len(tokenizer_de.get_vocab())\n",
        "    vocab_size_tgt = len(tokenizer_en.get_vocab())\n",
        "    \n",
        "    model = create_tensorflow_model(vocab_size_src, vocab_size_tgt, embedding_dim, hidden_units, dropout_rate)\n",
        "    \n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    # Callbacks\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "    model_checkpoint = ModelCheckpoint('best_model_tf.h5', monitor='val_loss', save_best_only=True)\n",
        "    \n",
        "    # Train model\n",
        "    history = model.fit(\n",
        "        train_ds_tf,\n",
        "        epochs=10,\n",
        "        validation_data=val_ds_tf,\n",
        "        callbacks=[early_stopping, model_checkpoint]\n",
        "    )\n",
        "    \n",
        "    # Evaluate model\n",
        "    test_ds_tf = prepare_tf_dataset(test_dataset)\n",
        "    _, test_accuracy = model.evaluate(test_ds_tf)\n",
        "    \n",
        "    return test_accuracy\n",
        "\n",
        "print(\"\\nRunning TensorFlow model hyperparameter tuning...\")\n",
        "study_tf = optuna.create_study(direction='maximize')\n",
        "study_tf.optimize(objective_tensorflow, n_trials=5)  # Adjust n_trials as needed\n",
        "\n",
        "best_params_tf = study_tf.best_params\n",
        "print(\"\\nBest TensorFlow hyperparameters:\")\n",
        "print(best_params_tf)\n",
        "print(f\"Best accuracy: {study_tf.best_value:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Melatih Model dengan Hyperparameter Terbaik"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train final TensorFlow model\n",
        "print(\"\\nTraining final TensorFlow model with best hyperparameters...\")\n",
        "vocab_size_src = len(tokenizer_de.get_vocab())\n",
        "vocab_size_tgt = len(tokenizer_en.get_vocab())\n",
        "\n",
        "model_tf = create_tensorflow_model(\n",
        "    vocab_size_src, vocab_size_tgt,\n",
        "    best_params_tf['embedding_dim'],\n",
        "    best_params_tf['hidden_units'],\n",
        "    best_params_tf['dropout_rate']\n",
        ")\n",
        "\n",
        "model_tf.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(best_params_tf['learning_rate']),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "train_ds_tf = prepare_tf_dataset(train_dataset)\n",
        "val_ds_tf = prepare_tf_dataset(val_dataset)\n",
        "test_ds_tf = prepare_tf_dataset(test_dataset)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('final_model_tf.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "history_tf = model_tf.fit(\n",
        "    train_ds_tf,\n",
        "    epochs=20,\n",
        "    validation_data=val_ds_tf,\n",
        "    callbacks=[early_stopping, model_checkpoint]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Mengevaluasi Model dengan Menghitung Akurasi, Presisi, Recall, F1Squared, ROC, dan AUC-ROC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluasi model RNN\n",
        "result_tf = evaluate_model_tf(model_tf, test_ds_tf, \"TensorFlow\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Memvisualisasikan Prediksi dari model yang telah dilatih"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotting hasil pelatihan\n",
        "plot_training_history(history_tf.history, \"TensorFlow\")\n",
        "\n",
        "# Plotting confusion matrix\n",
        "plot_confusion_matrix(result_tf['y_test'], result_tf['y_pred'], \"TensorFlow\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Menyimpan Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Menyimpan model\n",
        "model_tf.save('model_tf.keras')\n",
        "print(\"Model disimpan sebagai 'model_tf.keras'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perbandingan antara model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Menghitung perbandingan untuk semua model\n",
        "results_dict = {\n",
        "    'PyTorch': test_metrics_pytorch,\n",
        "    'TensorFlow': result_tf\n",
        "}\n",
        "\n",
        "# Plotting ROC curve\n",
        "plot_roc_curve(results_dict)\n",
        "# Plotting perbandingan metrik\n",
        "plot_metrics_comparison(results_dict)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "sEcuhK7kxBfZ"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dlvenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
