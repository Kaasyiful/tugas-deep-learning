{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 19. Training and Deploying TensorFlow Models at Scale**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menyajikan Model TensorFlow \n",
    "Setelah model TensorFlow dilatih, model tersebut perlu dimasukkan ke dalam produksi. Ini bisa sesederhana menjalankan model pada batch data, tetapi seringkali lebih terlibat, seperti membungkus model dalam layanan web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Pertama-tama, kita perlu mengimpor beberapa modul yang umum digunakan, memastikan Matplotlib menampilkan gambar secara inline, dan menyiapkan fungsi untuk menyimpan gambar. Kita juga memeriksa apakah Python 3.5 atau lebih baru terpasang (meskipun Python 2.x mungkin berfungsi, itu sudah tidak didukung lagi, jadi kami sangat menyarankan Anda menggunakan Python 3), serta Scikit-Learn ≥0.20 dan TensorFlow ≥2.0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Apakah notebook ini berjalan di Colab atau Kaggle?\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
    "\n",
    "if IS_COLAB or IS_KAGGLE:\n",
    "    !echo \"deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" > /etc/apt/sources.list.d/tensorflow-serving.list\n",
    "    !curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add -\n",
    "    !apt update && apt-get install -y tensorflow-model-server\n",
    "    %pip install -q -U tensorflow-serving-api\n",
    "\n",
    "# Diperlukan scikit-learn ≥0.20\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Diperlukan tensorflow ≥2.0\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "    if IS_KAGGLE:\n",
    "        print(\"Go to Settings > Accelerator and select GPU.\")\n",
    "\n",
    "# Impor umum\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# untuk membuat output notebook ini stabil di seluruh berjalan\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Untuk merencanakan angka cantik\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Di mana menyimpan angka\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deploy\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Menyebarkan model TensorFlow ke TensorFlow Serving (TFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF Serving adalah server model berperforma tinggi yang ditulis dalam C++. Ia dapat melayani berbagai versi model Anda, secara otomatis menerapkan versi terbaru, dan lainnya.\n",
    "* **Mengekspor SavedModels**:\n",
    "    * Model TensorFlow diekspor ke format **SavedModel** menggunakan `tf.saved_model.save(model, model_path)` atau `model.save(model_path)` (jika ekstensi file bukan `.h5`).\n",
    "    * SavedModel menyimpan grafik komputasi model dan bobotnya. Sebaiknya sertakan semua lapisan pra-pemrosesan dalam model akhir yang diekspor.\n",
    "    * SavedModel direpresentasikan sebagai direktori yang berisi file `saved_model.pb` (grafik komputasi) dan subdirektori `variables` (nilai variabel). Mungkin juga menyertakan subdirektori `assets` untuk data tambahan.\n",
    "    * SavedModel dapat dimuat menggunakan `tf.saved_model.load()` (mengembalikan objek SavedModel) atau `keras.models.load_model()` (mengembalikan model Keras).\n",
    "    * Alat baris perintah `saved_model_cli` dapat digunakan untuk memeriksa SavedModels dan membuat prediksi. SavedModel berisi satu atau lebih *metagraph*, yang merupakan grafik komputasi ditambah beberapa definisi tanda tangan fungsi.\n",
    "* **Menginstal TensorFlow Serving**: Opsi yang disarankan adalah menggunakan Docker.\n",
    "    ```bash\n",
    "    docker pull tensorflow/serving\n",
    "    docker run -it --rm -p 8500:8500 -p 8501:8501 \\\n",
    "      -v \"/path/to/my_mnist_model:/models/my_mnist_model\" \\\n",
    "      -e MODEL_NAME=my_mnist_model \\\n",
    "      tensorflow/serving\n",
    "    ```\n",
    "    Perintah ini menjalankan TF Serving, memuat model MNIST, dan menyajikannya melalui gRPC (port 8500) dan REST (port 8501).\n",
    "* **Mengirim Kueri ke TF Serving melalui REST API**:\n",
    "    * Kueri harus dalam format JSON dan berisi nama tanda tangan fungsi serta data inpu.\n",
    "    * Permintaan HTTP POST dikirim ke server. REST API sederhana tetapi bisa tidak efisien untuk data besar karena berbasis teks (JSON).\n",
    "* **Mengirim Kueri ke TF Serving melalui gRPC API**:\n",
    "    * Lebih efisien untuk data besar karena menggunakan format biner ringkas dan protokol komunikasi yang efisien (berbasis HTTP/2).\n",
    "    * Memerlukan *protobuf* `PredictRequest` sebagai input dan menghasilkan *protobuf* `PredictResponse`.\n",
    "    * Membutuhkan pustaka `tensorflow-serving-api` dan `grpcio`.\n",
    "* **Menerapkan Versi Model Baru**: TF Serving secara otomatis mendeteksi versi model baru di repositori model dan menangani transisi dengan baik. Ini dapat mengosongkan versi sebelumnya sebelum memuat yang baru atau memuat yang baru sambil tetap melayani permintaan yang tertunda dengan yang lama. TF Serving juga memiliki kemampuan *batching* otomatis (--enable_batching)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita akan menggunakan API REST atau API GRPC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simpan/muat `saveDmodel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "X_train_full = X_train_full[..., np.newaxis].astype(np.float32) / 255.\n",
    "X_test = X_test[..., np.newaxis].astype(np.float32) / 255.\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_new = X_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 1.1140 - accuracy: 0.7066 - val_loss: 0.3715 - val_accuracy: 0.9024\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 1s 713us/step - loss: 0.3695 - accuracy: 0.8981 - val_loss: 0.2990 - val_accuracy: 0.9144\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 1s 718us/step - loss: 0.3154 - accuracy: 0.9100 - val_loss: 0.2651 - val_accuracy: 0.9272\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 1s 706us/step - loss: 0.2765 - accuracy: 0.9223 - val_loss: 0.2436 - val_accuracy: 0.9334\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 1s 711us/step - loss: 0.2556 - accuracy: 0.9276 - val_loss: 0.2257 - val_accuracy: 0.9364\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 1s 715us/step - loss: 0.2367 - accuracy: 0.9321 - val_loss: 0.2121 - val_accuracy: 0.9396\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 1s 729us/step - loss: 0.2198 - accuracy: 0.9390 - val_loss: 0.1970 - val_accuracy: 0.9454\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 1s 716us/step - loss: 0.2057 - accuracy: 0.9425 - val_loss: 0.1880 - val_accuracy: 0.9476\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 1s 704us/step - loss: 0.1940 - accuracy: 0.9459 - val_loss: 0.1777 - val_accuracy: 0.9524\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 1s 711us/step - loss: 0.1798 - accuracy: 0.9482 - val_loss: 0.1684 - val_accuracy: 0.9546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe3b8718590>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28, 1]),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-2),\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.97, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(model.predict(X_new), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my_mnist_model/0001'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_version = \"0001\"\n",
    "model_name = \"my_mnist_model\"\n",
    "model_path = os.path.join(model_name, model_version)\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_mnist_model/0001/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_mnist_model/\n",
      "    0001/\n",
      "        saved_model.pb\n",
      "        variables/\n",
      "            variables.data-00000-of-00001\n",
      "            variables.index\n",
      "        assets/\n"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk(model_name):\n",
    "    indent = '    ' * root.count(os.sep)\n",
    "    print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "    for filename in files:\n",
    "        print('{}{}'.format(indent + '    ', filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel contains the following tag-sets:\r\n",
      "'serve'\r\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir {model_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel MetaGraphDef contains SignatureDefs with the following keys:\r\n",
      "SignatureDef key: \"__saved_model_init_op\"\r\n",
      "SignatureDef key: \"serving_default\"\r\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir {model_path} --tag_set serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel SignatureDef contains the following input(s):\r\n",
      "  inputs['flatten_input'] tensor_info:\r\n",
      "      dtype: DT_FLOAT\r\n",
      "      shape: (-1, 28, 28, 1)\r\n",
      "      name: serving_default_flatten_input:0\r\n",
      "The given SavedModel SignatureDef contains the following output(s):\r\n",
      "  outputs['dense_1'] tensor_info:\r\n",
      "      dtype: DT_FLOAT\r\n",
      "      shape: (-1, 10)\r\n",
      "      name: StatefulPartitionedCall:0\r\n",
      "Method name is: tensorflow/serving/predict\r\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir {model_path} --tag_set serve \\\n",
    "                      --signature_def serving_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['__saved_model_init_op']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['__saved_model_init_op'] tensor_info:\n",
      "        dtype: DT_INVALID\n",
      "        shape: unknown_rank\n",
      "        name: NoOp\n",
      "  Method name is: \n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['flatten_input'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 28, 28, 1)\n",
      "        name: serving_default_flatten_input:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['dense_1'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 10)\n",
      "        name: StatefulPartitionedCall:0\n",
      "  Method name is: tensorflow/serving/predict\n",
      "\n",
      "Defined Functions:\n",
      "  Function Name: '__call__'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          flatten_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='flatten_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "<<45 more lines>>\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #3\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          flatten_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='flatten_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #4\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          flatten_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='flatten_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir {model_path} --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mari kita tulis contoh baru ke file `npy` sehingga kita dapat meneruskannya dengan mudah ke model kita:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"my_mnist_tests.npy\", X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flatten_input'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_name = model.input_names[0]\n",
    "input_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dan sekarang mari kita gunakan `saved_model_cli` untuk membuat prediksi untuk contoh yang baru saja kita simpan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-18 22:15:30.294109: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-02-18 22:15:30.294306: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:tensorflow:From /Users/ageron/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_cli.py:445: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "INFO:tensorflow:Restoring parameters from my_mnist_model/0001/variables/variables\n",
      "2021-02-18 22:15:30.323498: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n",
      "Result for output key dense_1:\n",
      "[[1.1347984e-04 1.5187356e-07 9.7032893e-04 2.7640699e-03 3.7826971e-06\n",
      "  7.6876910e-05 3.9140293e-08 9.9559116e-01 5.3502394e-05 4.2665208e-04]\n",
      " [8.2443521e-04 3.5493889e-05 9.8826385e-01 7.0466995e-03 1.2957400e-07\n",
      "  2.3389691e-04 2.5639210e-03 9.5886099e-10 1.0314899e-03 8.7952529e-08]\n",
      " [4.4693781e-05 9.7028232e-01 9.0526715e-03 2.2641101e-03 4.8766597e-04\n",
      "  2.8800720e-03 2.2714981e-03 8.3753867e-03 4.0439744e-03 2.9759688e-04]]\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli run --dir {model_path} --tag_set serve \\\n",
    "                     --signature_def serving_default    \\\n",
    "                     --inputs {input_name}=my_mnist_tests.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.97, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round([[1.1347984e-04, 1.5187356e-07, 9.7032893e-04, 2.7640699e-03, 3.7826971e-06,\n",
    "           7.6876910e-05, 3.9140293e-08, 9.9559116e-01, 5.3502394e-05, 4.2665208e-04],\n",
    "          [8.2443521e-04, 3.5493889e-05, 9.8826385e-01, 7.0466995e-03, 1.2957400e-07,\n",
    "           2.3389691e-04, 2.5639210e-03, 9.5886099e-10, 1.0314899e-03, 8.7952529e-08],\n",
    "          [4.4693781e-05, 9.7028232e-01, 9.0526715e-03, 2.2641101e-03, 4.8766597e-04,\n",
    "           2.8800720e-03, 2.2714981e-03, 8.3753867e-03, 4.0439744e-03, 2.9759688e-04]], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instal [Docker] (https://docs.docker.com/install/) jika Anda belum memilikinya.Kemudian jalankan:\n",
    "\n",
    "```Bash\n",
    "Docker menarik tensorflow/serving\n",
    "ekspor ml_path = $ home/ml # atau di mana pun proyek ini berada\n",
    "Docker run -it --rm -p 8500: 8500 -p 8501: 8501 \\\n",
    "-v \"$ ml_path/my_mnist_model:/model/my_mnist_model\" \\\n",
    "-e model_name = my_mnist_model \\\n",
    "Tensorflow/serving\n",
    "```\n",
    "\n",
    "\n",
    "Setelah Anda selesai menggunakannya, tekan Ctrl-C untuk mematikan server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atau, jika `tensorflow_model_server` diinstal (mis., Jika Anda menjalankan buku catatan ini di Colab), maka 3 sel berikut akan memulai server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MODEL_DIR\"] = os.path.split(os.path.abspath(model_path))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "nohup tensorflow_model_server \\\n",
    "     --rest_api_port=8501 \\\n",
    "     --model_name=my_mnist_model \\\n",
    "     --model_base_path=\"${MODEL_DIR}\" >server.log 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-16 22:33:09.323538: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:93] Reading SavedModel debug info (if present) from: /models/my_mnist_model/0001\n",
      "2021-02-16 22:33:09.323642: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-02-16 22:33:09.360572: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:206] Restoring SavedModel bundle.\n",
      "2021-02-16 22:33:09.361764: I external/org_tensorflow/tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2200000000 Hz\n",
      "2021-02-16 22:33:09.387713: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:190] Running initialization op on SavedModel bundle at path: /models/my_mnist_model/0001\n",
      "2021-02-16 22:33:09.392739: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:277] SavedModel load for tags { serve }; Status: success: OK. Took 71106 microseconds.\n",
      "2021-02-16 22:33:09.393390: I tensorflow_serving/servables/tensorflow/saved_model_warmup_util.cc:59] No warmup data file found at /models/my_mnist_model/0001/assets.extra/tf_serving_warmup_requests\n",
      "2021-02-16 22:33:09.393847: I tensorflow_serving/core/loader_harness.cc:87] Successfully loaded servable version {name: my_mnist_model version: 1}\n",
      "2021-02-16 22:33:09.398470: I tensorflow_serving/model_servers/server.cc:371] Running gRPC ModelServer at 0.0.0.0:8500 ...\n",
      "[warn] getaddrinfo: address family for nodename not supported\n",
      "2021-02-16 22:33:09.405622: I tensorflow_serving/model_servers/server.cc:391] Exporting HTTP/REST API at:localhost:8501 ...\n",
      "[evhttp_server.cc : 238] NET_LOG: Entering the event loop ...\n"
     ]
    }
   ],
   "source": [
    "!tail server.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "input_data_json = json.dumps({\n",
    "    \"signature_name\": \"serving_default\",\n",
    "    \"instances\": X_new.tolist(),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\'{\"signature_name\": \"serving_default\", \"instances\": [[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.32941177487373...'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repr(input_data_json)[:1500] + \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekarang mari kita gunakan API REST TensorFlow Serving untuk membuat prediksi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "SERVER_URL = 'http://localhost:8501/v1/models/my_mnist_model:predict'\n",
    "response = requests.post(SERVER_URL, data=input_data_json)\n",
    "response.raise_for_status() # raise an exception in case of error\n",
    "response = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['predictions'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.97, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba = np.array(response[\"predictions\"])\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menggunakan API GRPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_serving.apis.predict_pb2 import PredictRequest\n",
    "\n",
    "request = PredictRequest()\n",
    "request.model_spec.name = model_name\n",
    "request.model_spec.signature_name = \"serving_default\"\n",
    "input_name = model.input_names[0]\n",
    "request.inputs[input_name].CopyFrom(tf.make_tensor_proto(X_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
    "\n",
    "channel = grpc.insecure_channel('localhost:8500')\n",
    "predict_service = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "response = predict_service.Predict(request, timeout=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "outputs {\n",
       "  key: \"dense_1\"\n",
       "  value {\n",
       "    dtype: DT_FLOAT\n",
       "    tensor_shape {\n",
       "      dim {\n",
       "        size: 3\n",
       "      }\n",
       "      dim {\n",
       "        size: 10\n",
       "      }\n",
       "    }\n",
       "    float_val: 0.00011425172124290839\n",
       "    float_val: 1.513665068841874e-07\n",
       "    float_val: 0.0009818424005061388\n",
       "    float_val: 0.0027773496694862843\n",
       "    float_val: 3.758880893656169e-06\n",
       "    float_val: 7.6266449468676e-05\n",
       "    float_val: 3.9139514740327286e-08\n",
       "    float_val: 0.995561957359314\n",
       "    float_val: 5.344580131350085e-05\n",
       "    float_val: 0.00043088122038170695\n",
       "    float_val: 0.0008194865076802671\n",
       "    float_val: 3.5498320357874036e-05\n",
       "    float_val: 0.9882420897483826\n",
       "    float_val: 0.00705744931474328\n",
       "    float_val: 1.2937064752804872e-07\n",
       "    float_val: 0.00023402832448482513\n",
       "    float_val: 0.0025743397418409586\n",
       "    float_val: 9.668431610876382e-10\n",
       "    float_val: 0.0010369382798671722\n",
       "    float_val: 8.833576004008137e-08\n",
       "    float_val: 4.441547571332194e-05\n",
       "    float_val: 0.970328688621521\n",
       "    float_val: 0.009044423699378967\n",
       "    float_val: 0.0022599005606025457\n",
       "    float_val: 0.00048672096454538405\n",
       "    float_val: 0.002873610006645322\n",
       "    float_val: 0.002268279204145074\n",
       "    float_val: 0.008354829624295235\n",
       "    float_val: 0.004041312728077173\n",
       "    float_val: 0.0002978229313157499\n",
       "  }\n",
       "}\n",
       "model_spec {\n",
       "  name: \"my_mnist_model\"\n",
       "  version {\n",
       "    value: 1\n",
       "  }\n",
       "  signature_name: \"serving_default\"\n",
       "}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konversi respons ke tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.97, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_name = model.output_names[0]\n",
    "outputs_proto = response.outputs[output_name]\n",
    "y_proba = tf.make_ndarray(outputs_proto)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atau ke array yang numpy jika klien Anda tidak menyertakan perpustakaan Tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.97, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_name = model.output_names[0]\n",
    "outputs_proto = response.outputs[output_name]\n",
    "shape = [dim.size for dim in outputs_proto.tensor_shape.dim]\n",
    "y_proba = np.array(outputs_proto.float_val).reshape(shape)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menyebarkan versi model baru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 1s 748us/step - loss: 1.1567 - accuracy: 0.6691 - val_loss: 0.3418 - val_accuracy: 0.9042\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 1s 697us/step - loss: 0.3376 - accuracy: 0.9032 - val_loss: 0.2674 - val_accuracy: 0.9242\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 1s 676us/step - loss: 0.2779 - accuracy: 0.9187 - val_loss: 0.2227 - val_accuracy: 0.9368\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 1s 669us/step - loss: 0.2362 - accuracy: 0.9318 - val_loss: 0.2032 - val_accuracy: 0.9432\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 1s 670us/step - loss: 0.2109 - accuracy: 0.9389 - val_loss: 0.1833 - val_accuracy: 0.9482\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 1s 675us/step - loss: 0.1951 - accuracy: 0.9430 - val_loss: 0.1740 - val_accuracy: 0.9498\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 1s 667us/step - loss: 0.1799 - accuracy: 0.9474 - val_loss: 0.1605 - val_accuracy: 0.9540\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 1s 673us/step - loss: 0.1654 - accuracy: 0.9519 - val_loss: 0.1543 - val_accuracy: 0.9558\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 1s 671us/step - loss: 0.1570 - accuracy: 0.9554 - val_loss: 0.1460 - val_accuracy: 0.9572\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 1s 672us/step - loss: 0.1420 - accuracy: 0.9583 - val_loss: 0.1359 - val_accuracy: 0.9616\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28, 1]),\n",
    "    keras.layers.Dense(50, activation=\"relu\"),\n",
    "    keras.layers.Dense(50, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-2),\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my_mnist_model/0002'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_version = \"0002\"\n",
    "model_name = \"my_mnist_model\"\n",
    "model_path = os.path.join(model_name, model_version)\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_mnist_model/0002/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_mnist_model/\n",
      "    0001/\n",
      "        saved_model.pb\n",
      "        variables/\n",
      "            variables.data-00000-of-00001\n",
      "            variables.index\n",
      "        assets/\n",
      "    0002/\n",
      "        saved_model.pb\n",
      "        variables/\n",
      "            variables.data-00000-of-00001\n",
      "            variables.index\n",
      "        assets/\n"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk(model_name):\n",
    "    indent = '    ' * root.count(os.sep)\n",
    "    print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "    for filename in files:\n",
    "        print('{}{}'.format(indent + '    ', filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** PERINGATAN **: Anda mungkin perlu menunggu sebentar sebelum model baru dimuat oleh tensorflow porsi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "SERVER_URL = 'http://localhost:8501/v1/models/my_mnist_model:predict'\n",
    "            \n",
    "response = requests.post(SERVER_URL, data=input_data_json)\n",
    "response.raise_for_status()\n",
    "response = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['predictions'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.99, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba = np.array(response[\"predictions\"])\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Menyebarkan model ke platform AI Google Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Membuat Layanan Prediksi di GCP AI Platform\n",
    "Google Cloud AI Platform dapat digunakan untuk menyajikan model TensorFlow. Langkah-langkah umumnya meliputi:\n",
    "1.  Menyiapkan akun GCP dan proyek, termasuk penagihan.\n",
    "2.  Membuat *bucket* Google Cloud Storage (GCS) untuk menyimpan SavedModels.\n",
    "3.  Mengunggah folder SavedModel ke *bucket* GCS.\n",
    "4.  Mengonfigurasi AI Platform dengan membuat model dan kemudian membuat versi model, menunjuk ke jalur SavedModel di GCS.\n",
    "5.  AI Platform menangani penskalaan otomatis dan penyeimbangan beban.\n",
    "6.  Untuk mengautentikasi aplikasi klien, **akun layanan** (*service account*) dengan hak akses terbatas (misalnya, peran ML Engine Developer) direkomendasikan daripada kredensial pengguna. Kunci privat akun layanan (file JSON) diunduh dan digunakan oleh aplikasi klien.\n",
    "7.  Pustaka Klien Google API dapat digunakan untuk berinteraksi dengan layanan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ikuti instruksi dalam buku ini untuk menggunakan model ke platform Google Cloud AI, unduh kunci pribadi akun layanan dan simpan ke `my_service_account_private_key.json` di direktori proyek.Juga, perbarui `Project_id`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"onyx-smoke-242003\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import googleapiclient.discovery\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"my_service_account_private_key.json\"\n",
    "model_id = \"my_mnist_model\"\n",
    "model_path = \"projects/{}/models/{}\".format(project_id, model_id)\n",
    "model_path += \"/versions/v0001/\" # if you want to run a specific version\n",
    "ml_resource = googleapiclient.discovery.build(\"ml\", \"v1\").projects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    input_data_json = {\"signature_name\": \"serving_default\",\n",
    "                       \"instances\": X.tolist()}\n",
    "    request = ml_resource.predict(name=model_path, body=input_data_json)\n",
    "    response = request.execute()\n",
    "    if \"error\" in response:\n",
    "        raise RuntimeError(response[\"error\"])\n",
    "    return np.array([pred[output_name] for pred in response[\"predictions\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.99, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_probas = predict(X_new)\n",
    "np.round(Y_probas, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Menerapkan Model ke Perangkat Seluler atau Tertanam (TensorFlow Lite - TFLite) \n",
    "TFLite membantu menerapkan model ke perangkat seluler dan tertanam dengan tujuan:\n",
    "* Mengurangi ukuran model (untuk mempersingkat waktu unduh dan mengurangi penggunaan RAM).\n",
    "* Mengurangi jumlah komputasi (untuk mengurangi latensi, penggunaan baterai, dan pemanasan).\n",
    "* Menyesuaikan model dengan batasan khusus perangka.\n",
    "* **Konverter Model TFLite**: Mengambil SavedModel dan mengompresnya ke format berbasis FlatBuffers (`.tflite`). Ini mengoptimalkan model dengan memangkas operasi yang tidak diperlukan dan menggabungkan operasi jika memungkinkan.\n",
    "* **Pengurangan Ukuran Model**:\n",
    "    * Menggunakan arsitektur jaringan saraf yang lebih kecil.\n",
    "    * Menggunakan lebar bit yang lebih kecil (misalnya, *half-float* 16-bit).\n",
    "    * **Kuantisasi**: Mengubah bobot menjadi bilangan bulat titik tetap 8-bi.\n",
    "        * ***Post-training quantization***: Mengkuantisasi bobot setelah pelatihan menggunakan teknik kuantisasi simetris. Mengurangi ukuran model secara signifikan, tetapi bobot yang dikuantisasi dikonversi kembali ke *float* saat *runtime* (kecuali jika aktivasi juga dikuantisasi).\n",
    "        * **Kuantisasi penuh**: Mengkuantisasi bobot dan aktivasi memungkinkan komputasi dilakukan sepenuhnya dengan bilangan bulat, menghasilkan percepatan yang signifikan. Memerlukan langkah kalibrasi menggunakan sampel data pelatihan yang representatif.\n",
    "        * ***Quantization-aware training***: Menambahkan operasi kuantisasi palsu ke model selama pelatihan sehingga model dapat belajar mengabaikan *noise* kuantisasi, menghasilkan bobot akhir yang lebih tahan terhadap kuantisasi.\n",
    "\n",
    "\n",
    "# TensorFlow di Browser (TensorFlow.js) \n",
    "Memungkinkan model berjalan langsung di browser pengguna. Berguna untuk konektivitas yang terputus-putus/lambat, respons latensi rendah, atau privasi pengguna.\n",
    "* Alat `tensorflowjs_converter` mengonversi SavedModel atau file model Keras ke format TensorFlow.js Layers (direktori berisi file bobot *sharded* dan file `model.json`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Menggunakan GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pelatihan jaringan saraf yang besar bisa sangat lambat. GPU dapat mempercepat pelatihan secara signifikan.\n",
    "* **Mendapatkan GPU**: Beli sendiri (kartu Nvidia dengan CUDA Compute Capability 3.5+) atau gunakan VM yang dilengkapi GPU di *cloud* (misalnya, Colaboratory).\n",
    "* Perlu menginstal driver Nvidia, CUDA, dan cuDNN (pustaka primitif yang dipercepat GPU untuk DNN).\n",
    "* **Mengelola RAM GPU**:\n",
    "    * Secara default, TensorFlow mengambil semua RAM di semua GPU yang tersedia.\n",
    "    * Variabel lingkungan `CUDA_VISIBLE_DEVICES` dapat digunakan untuk menetapkan GPU tertentu ke proses tertentu.\n",
    "    * `tf.config.experimental.set_virtual_device_configuration()` dengan `VirtualDeviceConfiguration(memory_limit=...)` untuk membatasi RAM yang diambil TensorFlow pada GPU.\n",
    "    * `tf.config.experimental.set_memory_growth(gpu, True)` agar TensorFlow hanya mengambil memori saat dibutuhkan.\n",
    "* **Menempatkan Operasi dan Variabel pada Perangkat**:\n",
    "    * TensorFlow memiliki algoritma penempatan dinamis (meskipun sekarang kurang digunakan), tetapi `tf.keras` dan `tf.data` umumnya melakukan penempatan yang baik (pra-pemrosesan data di CPU, operasi jaringan saraf di GPU).\n",
    "    * Penempatan manual dapat dilakukan menggunakan konteks `tf.device(\"/gpu:0\")` atau `tf.device(\"/cpu:0\")`.\n",
    "    * `tf.config.set_soft_device_placement(True)` memungkinkan *fallback* ke CPU jika penempatan GPU yang diminta gagal.\n",
    "* **Eksekusi Paralel Lintas Beberapa Perangkat**: TensorFlow menganalisis grafik TF Function untuk menemukan operasi yang dapat dijalankan secara paralel pada perangkat yang berbeda menggunakan antrean evaluasi dan *thread pool* (inter-op dan intra-op).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Catatan **: `tf.test.is_gpu_available ()` sudah usang.Sebagai gantinya, silakan gunakan `tf.config.list_physical_devices ('gpu')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.test.is_gpu_available () # usang\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 7325002731160755624,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 11139884032\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "     link {\n",
       "       device_id: 1\n",
       "       type: \"StreamExecutor\"\n",
       "       strength: 1\n",
       "     }\n",
       "   }\n",
       " }\n",
       " incarnation: 7150956550266107441\n",
       " physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\",\n",
       " name: \"/device:GPU:1\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 11139884032\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "     link {\n",
       "       type: \"StreamExecutor\"\n",
       "       strength: 1\n",
       "     }\n",
       "   }\n",
       " }\n",
       " incarnation: 15909479382059415698\n",
       " physical_device_desc: \"device: 1, name: Tesla K80, pci bus id: 0000:00:05.0, compute capability: 3.7\"]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client.device_lib import list_local_devices\n",
    "\n",
    "devices = list_local_devices()\n",
    "devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pelatihan Terdistribusi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Paralelisme Model**: Model dipecah menjadi beberapa bagian dan setiap bagian dijalankan pada perangkat yang berbeda. Sulit dan sangat bergantung pada arsitektur jaringan. Umumnya tidak banyak manfaat untuk jaringan yang terhubung penuh karena komunikasi lintas perangkat yang tinggi. Mungkin lebih baik untuk CNN atau RNN tertentu.\n",
    "* **Paralelisme Data**: Model direplikasi di setiap perangkat, dan setiap replika dilatih pada subset data yang berbeda. Gradien yang dihitung oleh setiap replika kemudian dirata-ratakan, dan hasilnya digunakan untuk memperbarui parameter model.\n",
    "    * **Strategi Tercermin (*Mirrored Strategy*)**: `tf.distribute.MirroredStrategy()`. Parameter model sepenuhnya dicerminkan di semua GPU, dan pembaruan parameter yang sama persis diterapkan pada setiap GPU (pembaruan sinkron). Menggunakan algoritma **AllReduce** (misalnya, NCCL) untuk menghitung rata-rata gradien secara efisien.\n",
    "    * **Parameter Terpusat**: `tf.distribute.experimental.CentralStorageStrategy()`. Parameter model disimpan di luar perangkat pekerja (misalnya, di CPU atau server parameter).\n",
    "        * ***Pembaruan Sinkron***: Agregator menunggu semua gradien sebelum menghitung rata-rata dan memperbarui parameter. Dapat diperlambat oleh replika yang paling lamba.\n",
    "        * ***Pembaruan Asinkron***: Setiap replika memperbarui parameter model secara independen segera setelah selesai menghitung gradien. Lebih banyak langkah pelatihan per menit tetapi dapat menderita **gradien basi** (*stale gradients*), yang dapat memperlambat konvergensi atau menyebabkan divergensi.\n",
    "    * **Saturasi *Bandwidth***: Menjadi masalah ketika mentransfer parameter dan gradien, terutama untuk model padat yang besar. Menggunakan presisi *float* 16-bit (`tf.bfloat16`) dapat membantu.\n",
    "* **API Strategi Distribusi**:\n",
    "    * `tf.distribute.MirroredStrategy()`: Untuk beberapa GPU di satu mesin (atau beberapa mesin dengan `MultiWorkerMirroredStrategy`).\n",
    "    * `tf.distribute.experimental.CentralStorageStrategy()`: Untuk parameter terpusat di satu mesin.\n",
    "    * `tf.distribute.experimental.ParameterServerStrategy()`: Untuk paralelisme data asinkron dengan server parameter di banyak mesin.\n",
    "    * `tf.distribute.experimental.TPUStrategy()`: Untuk TPU.\n",
    "* **Melatih Model di Klaster TensorFlow**:\n",
    "    * Klaster TensorFlow adalah sekelompok proses TensorFlow yang berjalan secara paralel, biasanya di mesin yang berbeda. Setiap proses TF disebut **tugas** (*task*) atau **server TF**, dengan alamat IP, port, dan **tipe** (misalnya, `\"worker\"`, `\"chief\"`, `\"ps\"`, `\"evaluator\"`).\n",
    "    * Spesifikasi klaster didefinisikan (misalnya, melalui variabel lingkungan `TF_CONFIG` dalam format JSON) untuk setiap tugas.\n",
    "* **Menjalankan Pekerjaan Pelatihan Besar di Google Cloud AI Platform**:\n",
    "    * AI Platform menangani penyediaan dan konfigurasi VM GPU.\n",
    "    * Gunakan alat baris perintah `gcloud` untuk mengirimkan pekerjaan pelatihan.\n",
    "    * **Penyetelan *Hyperparameter Black Box* di AI Platform (Google Vizier)**:\n",
    "        * Menggunakan file konfigurasi YAML untuk mendefinisikan ruang pencarian *hyperparameter*, tujuan (misalnya, `MAXIMIZE`), `hyperparameterMetricTag`, dll.\n",
    "        * AI Platform memantau direktori output untuk *event file* yang berisi metrik yang ditentukan dan menggunakan optimasi Bayesian untuk menyarankan nilai *hyperparameter* untuk uji coba berikutnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    return keras.models.Sequential([\n",
    "        keras.layers.Conv2D(filters=64, kernel_size=7, activation=\"relu\",\n",
    "                            padding=\"same\", input_shape=[28, 28, 1]),\n",
    "        keras.layers.MaxPooling2D(pool_size=2),\n",
    "        keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\",\n",
    "                            padding=\"same\"), \n",
    "        keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\",\n",
    "                            padding=\"same\"),\n",
    "        keras.layers.MaxPooling2D(pool_size=2),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(units=64, activation='relu'),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(units=10, activation='softmax'),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "550/550 [==============================] - 11s 18ms/step - loss: 1.8163 - accuracy: 0.3979 - val_loss: 0.3446 - val_accuracy: 0.9010\n",
      "Epoch 2/10\n",
      "550/550 [==============================] - 9s 17ms/step - loss: 0.4949 - accuracy: 0.8482 - val_loss: 0.1962 - val_accuracy: 0.9458\n",
      "Epoch 3/10\n",
      "550/550 [==============================] - 10s 17ms/step - loss: 0.3345 - accuracy: 0.9012 - val_loss: 0.1343 - val_accuracy: 0.9622\n",
      "Epoch 4/10\n",
      "550/550 [==============================] - 10s 17ms/step - loss: 0.2537 - accuracy: 0.9267 - val_loss: 0.1049 - val_accuracy: 0.9718\n",
      "Epoch 5/10\n",
      "550/550 [==============================] - 10s 17ms/step - loss: 0.2099 - accuracy: 0.9394 - val_loss: 0.0875 - val_accuracy: 0.9752\n",
      "Epoch 6/10\n",
      "550/550 [==============================] - 10s 17ms/step - loss: 0.1901 - accuracy: 0.9439 - val_loss: 0.0797 - val_accuracy: 0.9772\n",
      "Epoch 7/10\n",
      "550/550 [==============================] - 10s 18ms/step - loss: 0.1672 - accuracy: 0.9506 - val_loss: 0.0745 - val_accuracy: 0.9780\n",
      "Epoch 8/10\n",
      "550/550 [==============================] - 10s 18ms/step - loss: 0.1537 - accuracy: 0.9554 - val_loss: 0.0700 - val_accuracy: 0.9804\n",
      "Epoch 9/10\n",
      "550/550 [==============================] - 10s 18ms/step - loss: 0.1384 - accuracy: 0.9592 - val_loss: 0.0641 - val_accuracy: 0.9818\n",
      "Epoch 10/10\n",
      "550/550 [==============================] - 10s 18ms/step - loss: 0.1358 - accuracy: 0.9602 - val_loss: 0.0611 - val_accuracy: 0.9818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f014831c110>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "model = create_model()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-2),\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train, epochs=10,\n",
    "          validation_data=(X_valid, y_valid), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "distribution = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Ubah algoritma all-reduce default:\n",
    "# distribusi = tf.distribute.mirroredstrategy (\n",
    "# cross_device_ops = tf.distribute.hierarchicalCopyAlluce ())\n",
    "\n",
    "# Tentukan daftar GPU untuk digunakan:\n",
    "# distribusi = tf.distribute.mirroredstrategy (perangkat = [\"/gpu: 0\", \"/gpu: 1\"])\n",
    "\n",
    "# Gunakan strategi penyimpanan pusat sebagai gantinya:\n",
    "# distribusi = tf.distribute.experimental.centralstoragestry ()\n",
    "\n",
    "# if is_colab dan \"colab_tpu_addr\" di os.environ:\n",
    "# tpu_address = \"grpc: //\" + os.environ [\"colab_tpu_addr\"]\n",
    "# kalau tidak:\n",
    "# tpu_address = \"\"\n",
    "# resolver = tf.distribute.cluster_resolver.tpuclusterResolver (tpu_address)\n",
    "# tf.config.experimental_connect_to_cluster (resolver)\n",
    "# tf.tpu.experimental.initialize_tpu_system (resolver)\n",
    "# distribusi = tf.distribute.experimental.tpustrategy (resolver)\n",
    "\n",
    "with distribution.scope():\n",
    "    model = create_model()\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=keras.optimizers.SGD(learning_rate=1e-2),\n",
    "                  metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "INFO:tensorflow:batch_all_reduce: 10 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 10 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "550/550 [==============================] - 14s 16ms/step - loss: 1.8193 - accuracy: 0.3957 - val_loss: 0.3366 - val_accuracy: 0.9102\n",
      "Epoch 2/10\n",
      "550/550 [==============================] - 7s 13ms/step - loss: 0.4886 - accuracy: 0.8497 - val_loss: 0.1865 - val_accuracy: 0.9478\n",
      "Epoch 3/10\n",
      "550/550 [==============================] - 7s 13ms/step - loss: 0.3305 - accuracy: 0.9008 - val_loss: 0.1344 - val_accuracy: 0.9616\n",
      "Epoch 4/10\n",
      "550/550 [==============================] - 7s 13ms/step - loss: 0.2472 - accuracy: 0.9282 - val_loss: 0.1115 - val_accuracy: 0.9696\n",
      "Epoch 5/10\n",
      "550/550 [==============================] - 7s 13ms/step - loss: 0.2020 - accuracy: 0.9425 - val_loss: 0.0873 - val_accuracy: 0.9748\n",
      "Epoch 6/10\n",
      "550/550 [==============================] - 7s 13ms/step - loss: 0.1865 - accuracy: 0.9458 - val_loss: 0.0783 - val_accuracy: 0.9764\n",
      "Epoch 7/10\n",
      "550/550 [==============================] - 8s 14ms/step - loss: 0.1633 - accuracy: 0.9512 - val_loss: 0.0771 - val_accuracy: 0.9776\n",
      "Epoch 8/10\n",
      "550/550 [==============================] - 8s 14ms/step - loss: 0.1422 - accuracy: 0.9570 - val_loss: 0.0705 - val_accuracy: 0.9786\n",
      "Epoch 9/10\n",
      "550/550 [==============================] - 7s 13ms/step - loss: 0.1408 - accuracy: 0.9603 - val_loss: 0.0627 - val_accuracy: 0.9830\n",
      "Epoch 10/10\n",
      "550/550 [==============================] - 7s 13ms/step - loss: 0.1293 - accuracy: 0.9618 - val_loss: 0.0605 - val_accuracy: 0.9836\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f259bf1a6d0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100 # must be divisible by the number of workers\n",
    "model.fit(X_train, y_train, epochs=10,\n",
    "          validation_data=(X_valid, y_valid), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.53707055e-10, 7.94509292e-10, 1.02021443e-06, 3.37102080e-08,\n",
       "        4.90816797e-11, 4.37713789e-11, 2.43314297e-14, 9.99996424e-01,\n",
       "        1.50591750e-09, 2.50736753e-06],\n",
       "       [1.11715025e-07, 8.56921833e-05, 9.99914169e-01, 6.31697228e-09,\n",
       "        3.99949344e-11, 4.47976906e-10, 8.46022008e-09, 3.03771834e-08,\n",
       "        2.91782563e-08, 1.95555502e-10],\n",
       "       [4.68117065e-07, 9.99787748e-01, 1.01387537e-04, 2.87393277e-06,\n",
       "        5.29725839e-05, 1.55926125e-06, 2.07211669e-05, 1.76809226e-05,\n",
       "        9.37155255e-06, 5.19965897e-06]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop Pelatihan Kustom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "WARNING:tensorflow:From <ipython-input-9-acb7c62c8738>:36: DistributedIteratorV1.initialize (from tensorflow.python.distribute.input_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the iterator's `initializer` property instead.\n",
      "Epoch 1/10\n",
      "INFO:tensorflow:batch_all_reduce: 10 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 10 all-reduces with algorithm = nccl, num_packs = 1\n",
      "Loss: 0.380\n",
      "Epoch 2/10\n",
      "Loss: 0.302\n",
      "Epoch 3/10\n",
      "Loss: 0.285\n",
      "Epoch 4/10\n",
      "Loss: 0.294\n",
      "Epoch 5/10\n",
      "Loss: 0.304\n",
      "Epoch 6/10\n",
      "Loss: 0.310\n",
      "Epoch 7/10\n",
      "Loss: 0.310\n",
      "Epoch 8/10\n",
      "Loss: 0.306\n",
      "Epoch 9/10\n",
      "Loss: 0.303\n",
      "Epoch 10/10\n",
      "Loss: 0.298\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "K = keras.backend\n",
    "\n",
    "distribution = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with distribution.scope():\n",
    "    model = create_model()\n",
    "    optimizer = keras.optimizers.SGD()\n",
    "\n",
    "with distribution.scope():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).repeat().batch(batch_size)\n",
    "    input_iterator = distribution.make_dataset_iterator(dataset)\n",
    "    \n",
    "@tf.function\n",
    "def train_step():\n",
    "    def step_fn(inputs):\n",
    "        X, y = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            Y_proba = model(X)\n",
    "            loss = K.sum(keras.losses.sparse_categorical_crossentropy(y, Y_proba)) / batch_size\n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    per_replica_losses = distribution.experimental_run(step_fn, input_iterator)\n",
    "    mean_loss = distribution.reduce(tf.distribute.ReduceOp.SUM,\n",
    "                                    per_replica_losses, axis=None)\n",
    "    return mean_loss\n",
    "\n",
    "n_epochs = 10\n",
    "with distribution.scope():\n",
    "    input_iterator.initialize()\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, n_epochs))\n",
    "        for iteration in range(len(X_train) // batch_size):\n",
    "            print(\"\\rLoss: {:.3f}\".format(train_step().numpy()), end=\"\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pelatihan di beberapa server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster TensorFlow adalah sekelompok proses TensorFlow yang berjalan secara paralel, biasanya pada mesin yang berbeda, dan berbicara satu sama lain untuk menyelesaikan beberapa pekerjaan, misalnya melatih atau melaksanakan jaringan saraf.Setiap proses TF dalam cluster disebut \"tugas\" (atau \"server TF\").Ini memiliki alamat IP, port, dan jenis (juga disebut perannya atau tugasnya).Jenisnya bisa `\" pekerja \"`, `\" chief \"`, `\" ps \"` (server parameter) atau `\" evaluator \"`:\n",
    "*Masing -masing ** Pekerja ** melakukan perhitungan, biasanya pada mesin dengan satu atau lebih GPU.\n",
    "*The ** Chief ** melakukan perhitungan juga, tetapi juga menangani pekerjaan tambahan seperti menulis log tensorboard atau menyimpan pos pemeriksaan.Ada satu kepala dalam sebuah cluster, biasanya pekerja pertama (mis., Pekerja #0).\n",
    "*A ** Server Parameter ** (PS) hanya melacak nilai variabel, biasanya pada mesin hanya CPU.\n",
    "*Evaluator ** ** jelas menangani evaluasi.Biasanya ada evaluator tunggal dalam sebuah cluster.\n",
    "\n",
    "Set tugas yang berbagi jenis yang sama sering disebut \"pekerjaan\".Misalnya, pekerjaan \"pekerja\" adalah himpunan semua pekerja.\n",
    "\n",
    "Untuk memulai kluster TensorFlow, Anda harus terlebih dahulu mendefinisikannya.Ini berarti menentukan semua tugas (alamat IP, port TCP, dan jenis).Misalnya, spesifikasi cluster berikut mendefinisikan cluster dengan 3 tugas (2 pekerja dan 1 server parameter).Ini adalah kamus dengan satu kunci per pekerjaan, dan nilainya adalah daftar alamat tugas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_spec = {\n",
    "    \"worker\": [\n",
    "        \"machine-a.example.com:2222\",  # /job:worker/task:0\n",
    "        \"machine-b.example.com:2222\"   # /job:worker/task:1\n",
    "    ],\n",
    "    \"ps\": [\"machine-c.example.com:2222\"] # /job:ps/task:0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setiap tugas dalam cluster dapat berkomunikasi dengan setiap tugas lain di server, jadi pastikan untuk mengkonfigurasi firewall Anda untuk mengesahkan semua komunikasi antara mesin -mesin ini di port ini (biasanya lebih sederhana jika Anda menggunakan port yang sama di setiap mesin).\n",
    "\n",
    "Ketika suatu tugas dimulai, perlu diberitahu yang mana: tipe dan indeksnya (indeks tugas juga disebut ID tugas).Cara umum untuk menentukan semuanya sekaligus (baik spesifikasi cluster dan tipe dan ID tugas saat ini) adalah dengan mengatur variabel lingkungan `tf_config` sebelum memulai program.Ini harus menjadi kamus yang dikodekan JSON yang berisi spesifikasi cluster (di bawah `\" cluster \"` kunci), dan jenis dan indeks tugas untuk memulai (di bawah tugas `\" Tugas \"`).Misalnya, variabel lingkungan `tf_config` berikut mendefinisikan cluster yang sama seperti di atas, dengan 2 pekerja dan 1 server parameter, dan menentukan bahwa tugas untuk memulai adalah pekerja #1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"cluster\": {\"worker\": [\"machine-a.example.com:2222\", \"machine-b.example.com:2222\"], \"ps\": [\"machine-c.example.com:2222\"]}, \"task\": {\"type\": \"worker\", \"index\": 1}}'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "os.environ[\"TF_CONFIG\"] = json.dumps({\n",
    "    \"cluster\": cluster_spec,\n",
    "    \"task\": {\"type\": \"worker\", \"index\": 1}\n",
    "})\n",
    "os.environ[\"TF_CONFIG\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beberapa platform (mis., Google Cloud ML Engine) secara otomatis mengatur variabel lingkungan ini untuk Anda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kelas TensorFlow `TFConfigClusterResolver` membaca konfigurasi cluster dari variabel lingkungan ini:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClusterSpec({'ps': ['machine-c.example.com:2222'], 'worker': ['machine-a.example.com:2222', 'machine-b.example.com:2222']})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\n",
    "resolver.cluster_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'worker'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resolver.task_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resolver.task_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekarang mari kita jalankan cluster yang lebih sederhana dengan hanya dua tugas pekerja, keduanya berjalan di mesin lokal. kita akan menggunakan `multiworkermirroredstrategy` untuk melatih model di kedua tugas ini.\n",
    "\n",
    "Langkah pertama adalah menulis kode pelatihan.Karena kode ini akan digunakan untuk menjalankan kedua pekerja, masing -masing dalam prosesnya sendiri, kita menulis kode ini ke file python terpisah, `my_mnist_multiworker_task.py`.Kode ini relatif mudah, tetapi ada beberapa hal penting yang perlu diperhatikan:\n",
    "* kita membuat `multiworkerMirroredStrategy` sebelum melakukan hal lain dengan TensorFlow.\n",
    "* Hanya satu pekerja yang akan mengurus logging ke Tensorboard dan menyimpan pos pemeriksaan.Seperti disebutkan sebelumnya, pekerja ini disebut *kepala *, dan dengan konvensi biasanya pekerja #0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting my_mnist_multiworker_task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile my_mnist_multiworker_task.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time\n",
    "\n",
    "# Di awal program\n",
    "distribution = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "\n",
    "resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\n",
    "print(\"Starting task {}{}\".format(resolver.task_type, resolver.task_id))\n",
    "\n",
    "# Hanya pekerja #0 yang akan menulis pos pemeriksaan dan log ke tensorboard\n",
    "if resolver.task_id == 0:\n",
    "    root_logdir = os.path.join(os.curdir, \"my_mnist_multiworker_logs\")\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    run_dir = os.path.join(root_logdir, run_id)\n",
    "    callbacks = [\n",
    "        keras.callbacks.TensorBoard(run_dir),\n",
    "        keras.callbacks.ModelCheckpoint(\"my_mnist_multiworker_model.h5\",\n",
    "                                        save_best_only=True),\n",
    "    ]\n",
    "else:\n",
    "    callbacks = []\n",
    "\n",
    "# Muat dan persiapkan dataset MNIST\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "X_train_full = X_train_full[..., np.newaxis] / 255.\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "with distribution.scope():\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Conv2D(filters=64, kernel_size=7, activation=\"relu\",\n",
    "                            padding=\"same\", input_shape=[28, 28, 1]),\n",
    "        keras.layers.MaxPooling2D(pool_size=2),\n",
    "        keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\",\n",
    "                            padding=\"same\"), \n",
    "        keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\",\n",
    "                            padding=\"same\"),\n",
    "        keras.layers.MaxPooling2D(pool_size=2),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(units=64, activation='relu'),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(units=10, activation='softmax'),\n",
    "    ])\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=keras.optimizers.SGD(learning_rate=1e-2),\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "          epochs=10, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dalam aplikasi dunia nyata, biasanya akan ada satu pekerja per mesin, tetapi dalam contoh ini kita menjalankan kedua pekerja di mesin yang sama, sehingga mereka berdua akan mencoba menggunakan semua RAM GPU yang tersedia (jika mesin ini memiliki GPU), dan ini kemungkinan akan menyebabkan kesalahan out-of-memory (OOM).Untuk menghindari ini, kita dapat menggunakan variabel lingkungan `cuda_visible_devices` untuk menetapkan GPU yang berbeda untuk setiap pekerja.Atau, kita dapat menonaktifkan dukungan GPU, seperti ini:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kita sekarang siap untuk memulai kedua pekerja, masing -masing dalam prosesnya sendiri, menggunakan modul `Subprocess` Python.Sebelum kita memulai setiap proses, kita perlu mengatur variabel lingkungan `tf_config` dengan tepat, hanya mengubah indeks tugas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "cluster_spec = {\"worker\": [\"127.0.0.1:9901\", \"127.0.0.1:9902\"]}\n",
    "\n",
    "for index, worker_address in enumerate(cluster_spec[\"worker\"]):\n",
    "    os.environ[\"TF_CONFIG\"] = json.dumps({\n",
    "        \"cluster\": cluster_spec,\n",
    "        \"task\": {\"type\": \"worker\", \"index\": index}\n",
    "    })\n",
    "    subprocess.Popen(\"python my_mnist_multiworker_task.py\", shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Itu saja!Cluster TensorFlow kita sekarang berjalan, tetapi kita tidak dapat melihatnya di buku catatan ini karena berjalan dalam proses terpisah (tetapi jika Anda menjalankan buku catatan ini di Jupyter, Anda dapat melihat log pekerja di log server Jupyter).\n",
    "\n",
    "Karena Kepala (Pekerja #0) menulis ke Tensorboard, kita menggunakan Tensorboard untuk melihat kemajuan pelatihan.Jalankan sel berikut, lalu klik tombol Pengaturan (mis., Ikon Gear) di antarmuka Tensorboard dan centang kotak \"Data Muat Ulang\" untuk membuat papan tensor secara otomatis menyegarkan setiap 30 -an.Setelah zaman pertama pelatihan selesai (yang mungkin memakan waktu beberapa menit), dan setelah Tensorboard menyegarkan, tab Scalars akan muncul.Klik pada tab ini untuk melihat kemajuan pelatihan model dan akurasi validasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-5a57ac3228038e20\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-5a57ac3228038e20\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_mnist_multiworker_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Itu saja!Setelah pelatihan selesai, pos pemeriksaan terbaik dari model akan tersedia di file `my_mnist_multiworker_model.h5`.Anda dapat memuatnya menggunakan `keras.models.load_model ()` dan menggunakannya untuk prediksi, seperti biasa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.load_model(\"my_mnist_multiworker_model.h5\")\n",
    "Y_pred = model.predict(X_new)\n",
    "np.argmax(Y_pred, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dan itu saja untuk hari ini!Semoga Anda menemukan ini bermanfaat.😊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solusi Latihan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  **Isi SavedModel dan Cara Memeriksanya**: SavedModel berisi model TensorFlow, termasuk arsitekturnya (grafik komputasi) dan bobotnya.  Disimpan sebagai direktori yang berisi file `saved_model.pb` dan subdirektori `variables`.  Mungkin juga menyertakan subdirektori `assets`.  SavedModel dapat berisi satu atau lebih *metagraph* (grafik komputasi ditambah definisi tanda tangan fungsi).  Untuk memeriksanya, gunakan alat baris perintah `saved_model_cli` atau muat menggunakan `tf.saved_model.load()` dan periksa di Python. \n",
    "2.  **Kapan Menggunakan TF Serving dan Fitur Utamanya**: TF Serving memungkinkan Anda menerapkan beberapa model TensorFlow (atau beberapa versi dari model yang sama) dan membuatnya mudah diakses oleh semua aplikasi Anda melalui REST API atau gRPC API.  Fitur utamanya termasuk pemantauan direktori dan penerapan otomatis model baru, peralihan versi model yang mulus, dukungan untuk pengujian A/B dan penerapan terbatas (*canary*), kecepatan, pengujian yang baik, skalabilitas yang sangat baik, dan kemampuan untuk mengelompokkan permintaan individu menjadi batch.  Untuk menerapkannya, Anda dapat menggunakan gambar Docker atau solusi yang dihosting sepenuhnya seperti Google Cloud AI Platform. \n",
    "3.  **Menerapkan Model di Beberapa Instans TF Serving**: Yang perlu Anda lakukan hanyalah mengonfigurasi instans TF Serving ini untuk memantau direktori model yang sama, lalu mengekspor model baru Anda sebagai SavedModel ke subdirektori. \n",
    "4.  **Kapan Menggunakan gRPC API vs. REST API dengan TF Serving**: gRPC API lebih efisien daripada REST API.  Namun, pustaka kliennya tidak tersedia secara luas, dan jika Anda mengaktifkan kompresi saat menggunakan REST API, Anda bisa mendapatkan kinerja yang hampir sama.  Jadi, gRPC API paling berguna ketika Anda membutuhkan kinerja setinggi mungkin dan klien tidak terbatas pada REST API. \n",
    "5.  **Cara TFLite Mengurangi Ukuran Model**:\n",
    "    * Konverter mengoptimalkan SavedModel, menyusutkannya dan mengurangi latensinya dengan memangkas operasi yang tidak perlu dan menggabungkan operasi. \n",
    "    * Kuantisasi pasca-pelatihan secara dramatis mengurangi ukuran model. \n",
    "    * Menyimpan model yang dioptimalkan menggunakan format FlatBuffer, yang dapat dimuat langsung ke RAM tanpa penguraian, mengurangi waktu muat dan jejak memori. \n",
    "6.  **Pelatihan Sadar Kuantisasi (*Quantization-Aware Training*)**: Melibatkan penambahan operasi kuantisasi palsu ke model selama pelatihan.  Ini memungkinkan model untuk belajar mengabaikan *noise* kuantisasi; bobot akhir akan lebih kuat terhadap kuantisasi. \n",
    "7.  **Paralelisme Model vs. Paralelisme Data**:\n",
    "    * Paralelisme Model: Memecah model Anda menjadi beberapa bagian dan menjalankannya secara paralel di beberapa perangkat. \n",
    "    * Paralelisme Data: Membuat beberapa replika persis dari model Anda dan menerapkannya di beberapa perangkat.  Pada setiap iterasi selama pelatihan, setiap replika diberi batch data yang berbeda, dan ia menghitung gradien kerugian terhadap parameter model. \n",
    "    * Paralelisme data umumnya direkomendasikan karena memerlukan lebih sedikit komunikasi antar perangkat, lebih mudah diimplementasikan, dan berfungsi dengan cara yang sama untuk model apa pun. \n",
    "8.  **Strategi Distribusi untuk Pelatihan di Beberapa Server**:\n",
    "    * **MultiWorkerMirroredStrategy**: Melakukan paralelisme data cermin.  Model direplikasi di semua server dan perangkat yang tersedia, dan setiap replika mendapatkan batch data yang berbeda dan menghitung gradiennya sendiri.  Rata-rata gradien dihitung dan dibagikan menggunakan implementasi AllReduce terdistribusi (NCCL secara default), dan semua replika melakukan langkah *Gradient Descent* yang sama.  Ini adalah yang paling sederhana untuk digunakan dan berkinerja cukup baik. \n",
    "    * **ParameterServerStrategy**: Melakukan paralelisme data asinkron.  Model direplikasi di semua perangkat di semua pekerja, dan parameter dipecah di semua server parameter.  Setiap pekerja menjalankan loop pelatihannya sendiri secara asinkron.  Umumnya lebih lambat dari strategi sebelumnya. \n",
    "    * Anda harus menggunakan **MultiWorkerMirroredStrategy** secara umum.  Batasan utamanya adalah ia mengharuskan model muat dalam RAM pada setiap replika.  **ParameterServerStrategy** berguna untuk melatih model besar yang tidak muat dalam RAM GPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.\n",
    "_Exercise: Latih model (model apa pun yang Anda suka) dan sebarkan ke platform TF atau Google Cloud AI.Tulis kode klien untuk menanyakannya menggunakan API REST atau API GRPC.Perbarui model dan gunakan versi baru.Kode klien Anda sekarang akan meminta versi baru.Kembali ke versi pertama._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Harap ikuti langkah-langkah di <a href = \"#deploying-tensorflow-model-to-tensorflow-serving- (tfs)\"> Menyebarkan model tensorflow ke bagian tensorflow yang melayani </a> di atas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.\n",
    "_Exercise: Latih model apa pun di beberapa GPU pada mesin yang sama menggunakan `mirroredstrategy` (jika Anda tidak memiliki akses ke GPU, Anda dapat menggunakan colaboratory dengan runtime GPU dan membuat dua GPU virtual).Latih model lagi menggunakan `CentralStoragestry` dan bandingkan waktu pelatihan._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Harap ikuti langkah-langkah di bagian [Pelatihan Terdistribusi] (#-Training Terdistribusi) di atas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.\n",
    "_Exercise: Latih model kecil di platform Google Cloud AI, menggunakan Black Box Hyperparameter Tuning._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silakan ikuti instruksi di halaman 716-717 dari buku ini.Anda juga dapat membaca [halaman dokumentasi ini] (https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-oveview) dan melalui contoh di [blog yang bagus ini iniPOST] (https://towardsdatacience.com/how-to-do-bayesian-hyper-parameter-tuning-on-a-blackbox-model-882009552c6d) oleh Lak Lakshmanan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
